{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40544,
     "status": "ok",
     "timestamp": 1763671023506,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "sI-cyALUmzI7",
    "outputId": "a70c333e-6ac4-4953-9dda-e249882ebf99"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!unzip \"PATH_TO_YOUR_ZIP_FILE\" -d \"DESTINATION_DIRECTORY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 68414,
     "status": "ok",
     "timestamp": 1763671113126,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "EIHXiBI4nBjg"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "# ===== Standard Library =====\n",
    "import copy\n",
    "import gc  # Only if used later\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback  # Only if used later\n",
    "from collections import Counter, defaultdict\n",
    "from contextlib import nullcontext\n",
    "from typing import Dict, List, Optional\n",
    "# ===== Third-party =====\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  # Remove if not used in the notebook\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (accuracy_score, auc, classification_report, confusion_matrix,\n",
    "                             f1_score, precision_score, recall_score, roc_curve)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.cuda.amp import autocast, GradScaler  # Consolidated amp import\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import Optimizer\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "from skimage import color # Added for color space conversions\n",
    "\n",
    "# Optional: Add specific imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick  # For advanced tick formatting in plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763671113151,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "ezEa4ufBnM24"
   },
   "outputs": [],
   "source": [
    " # @title Configurations\n",
    "# --- Environment / device ---\n",
    "def get_device():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "# --- Environment flags ---\n",
    "ON_COLAB = os.path.exists(\"/content\")\n",
    "\n",
    "# --- Paths per environment ---\n",
    "if ON_COLAB:\n",
    "    BASE_DATASET_DIR = \"PATH_TO_YOUR_DATASET_ON_COLAB\"\n",
    "    RESULTS_DIR = 'PATH_TO_YOUR_RESULTS_DIR_ON_COLAB'\n",
    "    EMBED_NPY = \"PATH_TO_YOUR_EMBEDDINGS_ON_COLAB\"\n",
    "else:\n",
    "    BASE_DATASET_DIR = \"PATH_TO_YOUR_LOCAL_DATASET\"\n",
    "    RESULTS_DIR = 'PATH_TO_YOUR_LOCAL_RESULTS_DIR'\n",
    "    EMBED_NPY = \"PATH_TO_YOUR_LOCAL_EMBEDDINGS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763671113154,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "2RnoqRAlxErf"
   },
   "outputs": [],
   "source": [
    "# @title Load Images\n",
    "\n",
    "def load_img_from_dir(dir_path, max_images_per_class=None):\n",
    "    image_paths, labels = [], []\n",
    "\n",
    "    for class_name in sorted(os.listdir(dir_path)):\n",
    "        class_path = os.path.join(dir_path, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        count = 0\n",
    "        for img_name in os.listdir(class_path):\n",
    "            if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            if not os.path.isfile(img_path):\n",
    "                continue\n",
    "\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_name)\n",
    "            count += 1\n",
    "\n",
    "            if max_images_per_class and count >= max_images_per_class:\n",
    "                break\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(image_paths)} pre-filtered images.\")\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1763671113166,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "9bxyxsXgn5F6"
   },
   "outputs": [],
   "source": [
    "# @title Transformers\n",
    "\n",
    "# Refined transformations for each type of augmentation.\n",
    "def base_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def aggressive_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "        transforms.RandomAffine(degrees=15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.2)\n",
    "    ])\n",
    "\n",
    "def specific_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6),\n",
    "            transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.5, p=0.5)\n",
    "        ], p=0.9),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.3)\n",
    "    ])\n",
    "\n",
    "# Define the Class-Based Augmentation Schedule with more dynamic decisions\n",
    "class ClassBasedAugmentationSchedule:\n",
    "    def __init__(self, class_policy_map=None, num_classes=0):\n",
    "        self.class_policy_map = class_policy_map or {}\n",
    "        self.class_performance = {i: 1.0 for i in range(num_classes)}  # Initialize performance\n",
    "\n",
    "    def update_performance(self, y_true, y_pred):\n",
    "        recalls = recall_score(y_true, y_pred, average=None, labels=np.unique(y_true), zero_division=0)\n",
    "        recall_map = dict(zip(np.unique(y_true), recalls))\n",
    "\n",
    "        for class_idx in self.class_performance:\n",
    "            if class_idx in recall_map:\n",
    "                self.class_performance[class_idx] = recall_map[class_idx]\n",
    "        print(f\"üìä Updated Augmentation Performance Metrics: {self.class_performance}\")\n",
    "\n",
    "    def get_transform(self, epoch, class_label):\n",
    "        class_label = int(class_label)\n",
    "\n",
    "        # Use base policy for warmup (epochs < 5)\n",
    "        if epoch < 5:\n",
    "            return \"base_transform\"\n",
    "\n",
    "        # Get base policy for class\n",
    "        base_policy = self.class_policy_map.get(class_label, \"base_transform\")\n",
    "\n",
    "        # Dynamic augmentation strategy based on performance\n",
    "        if self.class_performance.get(class_label, 1.0) < 0.5:\n",
    "            print(f\"Applying more aggressive transform for class {class_label} due to low recall.\")\n",
    "            return \"aggressive_transform\"\n",
    "        elif self.class_performance.get(class_label, 1.0) < 0.75:\n",
    "            return \"specific_transform\"\n",
    "        return base_policy  # Default case (e.g., class-specific or basic transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1763671113183,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "PLI6K8stol3N"
   },
   "outputs": [],
   "source": [
    "# @title Custom Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    An optimized custom dataset class for image classification.\n",
    "    - Applies augmentations using the high-performance Albumentations library.\n",
    "    - Pre-processes and stores metadata in the __init__ method to avoid\n",
    "      re-reading data and to make epoch starts much faster.\n",
    "    - Handles training and validation/testing modes cleanly based on whether a\n",
    "      dynamic 'class_policy_map' or a static 'transform_name' is provided.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths,\n",
    "        labels,\n",
    "        metadata=None, # This 'metadata' is a list of color_metrics dicts from z_train\n",
    "        transform_name=None, # For validation/test, pass the name of the transform, e.g., \"standard_transform\"\n",
    "        include_skin_vec=False,\n",
    "        triplet_embedding_dict=None,\n",
    "        class_policy_map=None, # For training, pass the policy map\n",
    "        num_classes=None,\n",
    "    ):\n",
    "        self.triplet_embedding_dict = triplet_embedding_dict or {}\n",
    "        self.metadata_input = metadata if metadata is not None else [] # Renamed to avoid conflict\n",
    "        self.is_train = class_policy_map is not None # True if a policy map is provided (training mode)\n",
    "\n",
    "        # Initialize epoch, will be updated by set_epoch for dynamic transforms\n",
    "        self.epoch = 0\n",
    "\n",
    "        if self.is_train:\n",
    "            self.aug_schedule = ClassBasedAugmentationSchedule(class_policy_map, num_classes)\n",
    "        # Store the name for static transform (validation/test mode)\n",
    "        self.transform_name_for_val_test = transform_name\n",
    "\n",
    "        # Transformation map (from `VsCu1UxyW4p4`) - ensures these are defined globally\n",
    "        self.transform_map = {\n",
    "            \"base_transform\": base_transform(),\n",
    "            \"standard_transform\": base_transform(),\n",
    "            \"aggressive_transform\": aggressive_transform(),\n",
    "            \"specific_transform\": specific_transform(),\n",
    "        }\n",
    "\n",
    "        # --- Pre-processing Loop ---\n",
    "        # This loop runs once to gather all necessary data, making __getitem__ much faster.\n",
    "        self.data = []\n",
    "        print(\"Pre-processing and caching dataset metadata...\")\n",
    "        for i, (img_path, label) in enumerate(tqdm(zip(image_paths, labels), total=len(image_paths))):\n",
    "\n",
    "            embedding = self.triplet_embedding_dict.get(os.path.basename(img_path).lower())\n",
    "            if embedding is None:\n",
    "                # Use a zero tensor if an embedding is missing\n",
    "                embedding = np.zeros(512, dtype=np.float32)\n",
    "\n",
    "            # Retrieve the raw metadata (color_metrics dict) for this item\n",
    "            raw_meta_for_item = self.metadata_input[i] if i < len(self.metadata_input) else {}\n",
    "\n",
    "            skin_vec = np.zeros(12, dtype=np.float32)\n",
    "            if include_skin_vec and raw_meta_for_item:\n",
    "                try:\n",
    "                    # Assumes raw_meta_for_item is a dictionary of metrics\n",
    "                    skin_vec = build_skin_vector(raw_meta_for_item)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not build skin vector for {img_path}. Using zeros. Error: {e}\")\n",
    "\n",
    "            # Store all necessary components, including the new ones\n",
    "            self.data.append({\n",
    "                \"path\": img_path,\n",
    "                \"label\": label,\n",
    "                \"skin_vec\": skin_vec,\n",
    "                \"embedding\": embedding,\n",
    "                \"raw_metadata\": raw_meta_for_item, # Store the full color_metrics dict\n",
    "                \"mst_bin\": raw_meta_for_item.get(\"MST\", 0), # Extract MST bin\n",
    "                \"skin_group\": bin_mst_to_skin_group(raw_meta_for_item.get(\"MST\", 0)) # Extract skin group\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the pre-processed data for the item\n",
    "        item_data = self.data[idx]\n",
    "        img_path = item_data[\"path\"]\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\") # PIL Image\n",
    "\n",
    "            # 2. Determine and apply the correct transform\n",
    "            if self.is_train:\n",
    "                # Dynamic transform for training\n",
    "                transform_key = self.aug_schedule.get_transform(self.epoch, item_data[\"label\"])\n",
    "                current_transform = self.transform_map.get(transform_key, base_transform())\n",
    "            else:\n",
    "                # Static transform for validation/test\n",
    "                transform_key = self.transform_name_for_val_test if self.transform_name_for_val_test else \"base_transform\"\n",
    "                current_transform = self.transform_map.get(transform_key, base_transform())\n",
    "\n",
    "            img_tensor = current_transform(img) # Apply torchvision transform to PIL Image\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  ERROR: Failed to load or process image {img_path}: {e}\")\n",
    "            # On error, return a placeholder tensor to prevent the training loop from crashing\n",
    "            return (\n",
    "                torch.zeros(3, 224, 224),\n",
    "                torch.tensor(item_data[\"label\"]),\n",
    "                torch.zeros(12),\n",
    "                torch.zeros(512),\n",
    "                torch.tensor(item_data[\"mst_bin\"], dtype=torch.int64),\n",
    "                item_data[\"skin_group\"],\n",
    "                item_data[\"raw_metadata\"]\n",
    "            )\n",
    "\n",
    "        # Convert numpy arrays to tensors\n",
    "        skin_vec_tensor = torch.tensor(item_data[\"skin_vec\"], dtype=torch.float32)\n",
    "        embedding = item_data[\"embedding\"]\n",
    "        embedding_tensor = embedding if isinstance(embedding, torch.Tensor) else torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "        # Retrieve the additional items to return\n",
    "        mst_bin_val = torch.tensor(item_data[\"mst_bin\"], dtype=torch.int64)\n",
    "        skin_group_val = item_data[\"skin_group\"]\n",
    "        raw_meta_dict = item_data[\"raw_metadata\"]\n",
    "\n",
    "        # Return 7 items as expected by evaluate_model\n",
    "        return img_tensor, item_data[\"label\"], skin_vec_tensor, embedding_tensor, mst_bin_val, skin_group_val, raw_meta_dict\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Sets the current epoch for the dataset, used by the augmentation schedule.\n",
    "        \"\"\"\n",
    "        self.epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1763671113197,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "Sgo-157GoHfD"
   },
   "outputs": [],
   "source": [
    "# @title Color Calculation\n",
    "\n",
    "def bin_mst_to_skin_group(mst_value: int) -> str:\n",
    "    return f\"MST_{mst_value}\" if 1 <= mst_value <= 10 else \"unknown\"\n",
    "\n",
    "def normalize_color_features(L, h):\n",
    "    L_scaled = L / 100.0\n",
    "    h_scaled = h / 360.0\n",
    "    return L_scaled, h_scaled\n",
    "\n",
    "def extract_color_metrics(image_path):\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    if image_bgr is None:\n",
    "        return None, None\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb = image_rgb / 255.0\n",
    "\n",
    "    lab = color.rgb2lab(image_rgb)\n",
    "    l = lab[:, :, 0]\n",
    "    a = lab[:, :, 1]\n",
    "    b = lab[:, :, 2]\n",
    "    h = np.degrees(np.arctan2(b, a)) % 360\n",
    "\n",
    "    skin_pixels = l > 0\n",
    "    avg_L = np.mean(l[skin_pixels])\n",
    "    avg_h = np.mean(h[skin_pixels])\n",
    "\n",
    "    return avg_L, avg_h\n",
    "\n",
    "def estimate_mst_from_ita(ita_value):\n",
    "    if ita_value > 55: return 1\n",
    "    elif ita_value > 41: return 2\n",
    "    elif ita_value > 28: return 3\n",
    "    elif ita_value > 19: return 4\n",
    "    elif ita_value > 10: return 5\n",
    "    elif ita_value > 0: return 6\n",
    "    elif ita_value > -10: return 7\n",
    "    elif ita_value > -20: return 8\n",
    "    elif ita_value > -30: return 9\n",
    "    else: return 10\n",
    "\n",
    "def extract_color_metrics_and_estimate_mst(image_path):\n",
    "    avg_L, avg_h = extract_color_metrics(image_path)\n",
    "    if avg_L is None or avg_h is None:\n",
    "        return None\n",
    "\n",
    "    ita = np.degrees(np.arctan((avg_L - 50) / avg_h))\n",
    "    mst_bin = estimate_mst_from_ita(ita)\n",
    "\n",
    "    return {\n",
    "        \"L\": avg_L,\n",
    "        \"h\": avg_h,\n",
    "        \"MST\": mst_bin\n",
    "    }\n",
    "\n",
    "def normalize_ita_hue(ita, hue):\n",
    "    ita_scaled = (ita + 60) / 120\n",
    "    hue_scaled = hue / 360.0\n",
    "    return ita_scaled, hue_scaled\n",
    "\n",
    "def one_hot_encode_mst(mst_bin, num_classes=10):\n",
    "    one_hot = np.zeros(num_classes)\n",
    "    if 1 <= mst_bin <= num_classes:\n",
    "        one_hot[mst_bin - 1] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def build_skin_vector(color_metrics):\n",
    "    if color_metrics is None:\n",
    "        return None\n",
    "\n",
    "    L = color_metrics[\"L\"]\n",
    "    h = color_metrics[\"h\"]\n",
    "    ita = np.degrees(np.arctan((L - 50) / h))\n",
    "    ita_scaled, hue_scaled = normalize_ita_hue(ita, h)\n",
    "\n",
    "    mst_onehot = one_hot_encode_mst(color_metrics[\"MST\"])\n",
    "    return np.array([ita_scaled, hue_scaled], dtype=np.float32).tolist() + mst_onehot.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763671113200,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "sLZ8VNrLpn63"
   },
   "outputs": [],
   "source": [
    "# @title Oversampling and Balancing\n",
    "\n",
    "# Dynamic oversampling target counts\n",
    "def calculate_dynamic_target_counts(y_encoded, z, oversample_percentage=1.2):\n",
    "    combo_counts = defaultdict(int)\n",
    "    for label_encoded, metadata in zip(y_encoded, z):\n",
    "        mst_group = bin_mst_to_skin_group(metadata.get(\"MST\")) # Use the same binning as balance_data_to_targets\n",
    "        if mst_group != \"unknown\": # Only consider known MST groups\n",
    "            combo_counts[(label_encoded, mst_group)] += 1\n",
    "\n",
    "    if not combo_counts:\n",
    "        return {} # Return empty if no valid combos found\n",
    "\n",
    "    max_combo_count = max(combo_counts.values())\n",
    "\n",
    "    dynamic_target_counts = {}\n",
    "    for (label_encoded, mst_group), count in combo_counts.items():\n",
    "        # Target all combos to be at least (max_combo_count * oversample_percentage)\n",
    "        dynamic_target_counts[(label_encoded, mst_group)] = int(max_combo_count * oversample_percentage)\n",
    "\n",
    "    return dynamic_target_counts\n",
    "\n",
    "# Definition for balance_data_to_targets (copied for self-sufficiency)\n",
    "def balance_data_to_targets(X, y, z, target_counts):\n",
    "    print(\"\\n‚öñÔ∏è Balancing dataset to meet fairness targets...\")\n",
    "\n",
    "    # Print class distribution before oversampling\n",
    "    print(\"\\nüìä Class Distribution Before Oversampling:\")\n",
    "    pre_oversample_distribution = defaultdict(Counter)\n",
    "    for label, metadata in zip(y, z):\n",
    "        mst_value = metadata.get(\"MST\")\n",
    "        if mst_value is not None:\n",
    "            pre_oversample_distribution[label][mst_value] += 1\n",
    "\n",
    "    for label, mst_counts in sorted(pre_oversample_distribution.items()):\n",
    "        print(f\"\\n--- Class: {label} ---\")\n",
    "        for mst_value, count in sorted(mst_counts.items()):\n",
    "            print(f\"  MST {mst_value}: {count} samples\")\n",
    "\n",
    "    grouped_indices = defaultdict(list)\n",
    "    for i, (label, metadata) in enumerate(zip(y, z)):\n",
    "        mst_group = bin_mst_to_skin_group(metadata.get(\"MST\"))\n",
    "        if mst_group != \"unknown\":\n",
    "            grouped_indices[(label, mst_group)].append(i)\n",
    "\n",
    "    balanced_indices = []\n",
    "    for (label, mst_group), target_count in target_counts.items():\n",
    "        available_indices = grouped_indices.get((label, mst_group), [])\n",
    "        if not available_indices:\n",
    "            continue\n",
    "\n",
    "        if target_count > len(available_indices):\n",
    "            chosen_indices = random.choices(available_indices, k=target_count)\n",
    "        else:\n",
    "            chosen_indices = random.sample(available_indices, k=target_count)\n",
    "\n",
    "        balanced_indices.extend(chosen_indices)\n",
    "\n",
    "    X_bal = [X[i] for i in balanced_indices]\n",
    "    y_bal = [y[i] for i in balanced_indices]\n",
    "    z_bal = [z[i] for i in balanced_indices]\n",
    "\n",
    "    # Print class distribution after oversampling\n",
    "    print(\"\\nüìä Class Distribution After Oversampling:\")\n",
    "    post_oversample_distribution = defaultdict(Counter)\n",
    "    for label, metadata in zip(y_bal, z_bal):\n",
    "        mst_value = metadata.get(\"MST\")\n",
    "        if mst_value is not None:\n",
    "            post_oversample_distribution[label][mst_value] += 1\n",
    "\n",
    "    for label, mst_counts in sorted(post_oversample_distribution.items()):\n",
    "        print(f\"\\n--- Class: {label} ---\")\n",
    "        for mst_value, count in sorted(mst_counts.items()):\n",
    "            print(f\"  MST {mst_value}: {count} samples\")\n",
    "\n",
    "    # Ensure that the dataset is not empty after balancing\n",
    "    if len(X_bal) == 0 or len(y_bal) == 0 or len(z_bal) == 0:\n",
    "        raise ValueError(\"Oversampled dataset is empty. Please check your oversampling logic.\")\n",
    "\n",
    "    return X_bal, y_bal, z_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1763671113230,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "LCu1Jq_cp7tg"
   },
   "outputs": [],
   "source": [
    "# @title Grad-Cam\n",
    "\n",
    "# Utility to clear all forward hooks to prevent retain_grad issues\n",
    "def clear_all_forward_hooks(model: torch.nn.Module):\n",
    "    \"\"\"Clear all forward hooks to prevent memory leaks and prevent retain_grad.\"\"\"\n",
    "    if hasattr(model, \"_forward_hooks\"):\n",
    "        model._forward_hooks.clear()\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"_forward_hooks\"):\n",
    "            m._forward_hooks.clear()\n",
    "\n",
    "class SafeGradCAM:\n",
    "    \"\"\"Safe Grad-CAM++ implementation that uses hooks to calculate and visualize gradients and activations for Grad-CAM++ heatmaps.\"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, target_layer: torch.nn.Module):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self._handles = []\n",
    "\n",
    "    def _fw_hook(self, module, inputs, output):\n",
    "        \"\"\"Forward hook to capture activations.\"\"\"\n",
    "        self.activations = output\n",
    "        if isinstance(output, torch.Tensor) and output.requires_grad:\n",
    "            output.retain_grad()  # Retain gradient if required\n",
    "\n",
    "    def _bw_hook(self, module, grad_input, grad_output):\n",
    "        \"\"\"Backward hook to capture gradients.\"\"\"\n",
    "        self.gradients = grad_output[0]  # Gradients w.r.t the target layer\n",
    "\n",
    "    def _register(self):\n",
    "        \"\"\"Register hooks to capture gradients and activations.\"\"\"\n",
    "        self._handles.append(self.target_layer.register_forward_hook(self._fw_hook))\n",
    "        self._handles.append(self.target_layer.register_full_backward_hook(self._bw_hook))\n",
    "\n",
    "    def _remove(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for h in self._handles:\n",
    "            try:\n",
    "                h.remove()\n",
    "            except Exception:\n",
    "                pass\n",
    "        self._handles = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _overlay(self, heatmap: np.ndarray, image_path: str):\n",
    "        \"\"\"Overlay the Grad-CAM++ heatmap onto the original image.\"\"\"\n",
    "        src = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if src is None:\n",
    "            return None\n",
    "        hm = (255 * (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-6)).astype(np.uint8)\n",
    "        hm = cv2.applyColorMap(hm, cv2.COLORMAP_JET)\n",
    "        out = cv2.addWeighted(src, 0.5, hm, 0.5, 0)\n",
    "        return out\n",
    "\n",
    "    def generate(self, input_tensor: torch.Tensor, skin_vec: torch.Tensor, target_class: int = None):\n",
    "        \"\"\"Generate Grad-CAM++ heatmap for a single image.\"\"\"\n",
    "        self.model.eval()\n",
    "        self._register()\n",
    "\n",
    "        try:\n",
    "            with torch.enable_grad():\n",
    "                input_tensor = input_tensor.requires_grad_(True)\n",
    "                out = self.model(input_tensor, skin_vec)\n",
    "                if target_class is None:\n",
    "                    target_class = int(out.argmax(dim=1).item())\n",
    "                loss = out[0, target_class]\n",
    "                self.model.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                A = self.activations   # [1,C,H,W]\n",
    "                G = self.gradients     # [1,C,H,W]\n",
    "                assert A is not None and G is not None, \"Hooks did not capture activations/gradients.\"\n",
    "\n",
    "                posG = F.relu(G)\n",
    "                alpha_num = G.pow(2)\n",
    "                alpha_den = 2 * alpha_num + (A * G.pow(3)).sum(dim=(2,3), keepdim=True)\n",
    "                alpha_den = torch.where(alpha_den != 0, alpha_den, torch.ones_like(alpha_den))\n",
    "                alpha = alpha_num / alpha_den\n",
    "                weights = (alpha * posG).sum(dim=(2,3))   # [1,C]\n",
    "                cam = (weights.unsqueeze(-1).unsqueeze(-1) * A).sum(dim=1)[0]  # [H,W]\n",
    "                cam = F.relu(cam)\n",
    "                cam = cam / (cam.max() + 1e-6)\n",
    "                return cam.detach().cpu().numpy()\n",
    "        finally:\n",
    "            self._remove()\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"Grad-CAM++ style heatmap for a specific target layer.\"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, target_layer: torch.nn.Module):\n",
    "        self.model = model.eval()\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None   # A\n",
    "        self.gradients = None     # dY/dA\n",
    "\n",
    "        # keep handles so you can remove later if needed\n",
    "        self._h_fwd = target_layer.register_forward_hook(self._fwd_hook)\n",
    "        self._h_bwd = target_layer.register_full_backward_hook(self._bwd_hook)\n",
    "\n",
    "    def _fwd_hook(self, module, inputs, output):\n",
    "        # DO NOT detach; we need the graph. Also, retain grad on activations.\n",
    "        self.activations = output\n",
    "        if isinstance(self.activations, torch.Tensor):\n",
    "            self.activations.retain_grad()\n",
    "\n",
    "    def _bwd_hook(self, module, grad_input, grad_output):\n",
    "        # grad_output is a tuple; take the gradient w.r.t. the layer output\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    @torch.enable_grad()  # make sure grads are enabled even during eval sections\n",
    "    def generate(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor,\n",
    "        skin_vec: torch.Tensor,\n",
    "        target_class: Optional[int] = None,\n",
    "        use_amp: bool = False,\n",
    "        device_type: str = \"cuda\",\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Returns a (H, W) numpy heatmap in [0,1].\"\"\"\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp:\n",
    "            from torch.amp import autocast\n",
    "            ctx = autocast(device_type=device_type, dtype=torch.float16)\n",
    "        else:\n",
    "            from contextlib import nullcontext\n",
    "            ctx = nullcontext()\n",
    "\n",
    "        with ctx:\n",
    "            output = self.model(input_tensor, skin_vec)  # [1, C]\n",
    "            if target_class is None:\n",
    "                target_class = int(output.argmax(dim=1).item())\n",
    "            score = output[0, target_class]\n",
    "\n",
    "        # Backprop dY/dA\n",
    "        score.backward(retain_graph=False)\n",
    "\n",
    "        A = self.activations            # [B, K, H, W]\n",
    "        dYdA = self.gradients           # [B, K, H, W]\n",
    "        assert A is not None and dYdA is not None, \"Hooks did not capture activations/gradients.\"\n",
    "\n",
    "        # Grad-CAM++ weights\n",
    "        dYdA_pos = F.relu(dYdA)\n",
    "        alpha_num = dYdA.pow(2)\n",
    "        alpha_den = 2 * alpha_num + (A * dYdA.pow(3)).sum(dim=(2, 3), keepdim=True)\n",
    "        alpha_den = torch.where(alpha_den != 0, alpha_den, torch.ones_like(alpha_den))\n",
    "        alpha = alpha_num / alpha_den\n",
    "        weights = (alpha * dYdA_pos).sum(dim=(2, 3))  # [B, K]\n",
    "\n",
    "        # Weighted sum of activation maps (use sample 0)\n",
    "        A0 = A[0]                     # [K, H, W]\n",
    "        w0 = weights[0].view(-1, 1, 1)\n",
    "        heatmap = (w0 * A0).sum(dim=0)  # [H, W]\n",
    "\n",
    "        heatmap = F.relu(heatmap)\n",
    "        heatmap = heatmap / (heatmap.max() + 1e-6)\n",
    "        return heatmap.detach().cpu().numpy()\n",
    "\n",
    "def get_gradcam_layer(model, model_name, use_gradcam=False):\n",
    "    \"\"\"Heuristically find the last convolutional layer for Grad-CAM.\"\"\"\n",
    "    if not use_gradcam:\n",
    "        print(\"Skipping Grad-CAM for this run.\")\n",
    "        return None  # Simply return None if Grad-CAM is not needed\n",
    "\n",
    "    # Existing Grad-CAM logic for model with layers (EfficientNet, etc.)\n",
    "    if hasattr(model, \"get_gradcam_target_layer\"):\n",
    "        return model.get_gradcam_target_layer()\n",
    "\n",
    "    # Handle EfficientNet models specifically\n",
    "    if isinstance(model, EfficientNetWithAttention):\n",
    "        # EfficientNet models might have features instead of conv_head\n",
    "        if hasattr(model, \"conv_head\"):\n",
    "            return model.conv_head  # In case conv_head exists for Grad-CAM\n",
    "        elif hasattr(model, \"features\"):\n",
    "            # Handle EfficientNet's features (the last layer in features)\n",
    "            return model.features[-1]  # EfficientNet models usually have the last convolutional layer in features\n",
    "        else:\n",
    "            raise ValueError(f\"EfficientNet model '{model_name}' does not have a conv_head or features layer for Grad-CAM.\")\n",
    "\n",
    "    # EfficientNet model family check (in case EfficientNet isn't using EfficientNetWithAttention)\n",
    "    if \"efficientnet\" in model_name.lower():\n",
    "        if hasattr(model, \"features\"):\n",
    "            return model.features[-1]  # The last layer in EfficientNet is usually in 'features'\n",
    "        else:\n",
    "            raise ValueError(f\"EfficientNet model '{model_name}' does not have the expected layer for Grad-CAM.\")\n",
    "\n",
    "    # Define layers for other model families\n",
    "    layers_map = {\n",
    "        \"resnet\": lambda model: model.base.layer4,  # ResNet: The last block of layer4\n",
    "        \"vgg\": lambda model: model.base.features[-1],  # VGG: The last feature layer\n",
    "        \"mobilenet\": lambda model: model.base.features[-1],  # MobileNet: The last feature layer\n",
    "        \"inception_v3\": lambda model: model.base.Mixed_7c,  # Inception V3: Last mixed block\n",
    "    }\n",
    "\n",
    "    # Check which model family we are dealing with\n",
    "    for model_prefix, get_layer_func in layers_map.items():\n",
    "        if model_name.lower().startswith(model_prefix):\n",
    "            try:\n",
    "                return get_layer_func(model)\n",
    "            except AttributeError as e:\n",
    "                raise ValueError(f\"Model '{model_name}' does not have the expected attribute for Grad-CAM: {e}\")\n",
    "\n",
    "    # If model name is not recognized\n",
    "    raise ValueError(f\"Unsupported model for Grad-CAM: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1763671113266,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "m7wIDkgyq34V"
   },
   "outputs": [],
   "source": [
    "# @title Multi Layer Perceptron\n",
    "\n",
    "class Skin_Multi_Layer_Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim=12, hidden_dim1=32, hidden_dim2=16, output_dim=8):\n",
    "        super(Skin_Multi_Layer_Perceptron, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.BatchNorm1d(hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.BatchNorm1d(hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim2, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1763671113282,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "EGxaVRZ2tFqo"
   },
   "outputs": [],
   "source": [
    "# @title Two Layer Classifier Head\n",
    "\n",
    "class TwoLayerClassifierHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, output_dim=7):  # or 4 for FairFace4\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1763671113300,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "aq4627Vxtpkx"
   },
   "outputs": [],
   "source": [
    "# @title CBAM\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7, use_film=False, film_in_dim=12):\n",
    "        super().__init__()\n",
    "        assert isinstance(channels, int) and channels > 0, \\\n",
    "            f\"‚ùå CBAM init error: channels must be int>0, got {channels}\"\n",
    "\n",
    "        self.channels = channels\n",
    "        self.use_film = use_film\n",
    "\n",
    "        # Channel Attention\n",
    "        self.channel_attn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Spatial Attention\n",
    "        self.spatial_attn = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Optional FiLM\n",
    "        if self.use_film:\n",
    "            self.film = FiLM(film_in_dim, channels)\n",
    "\n",
    "    def forward(self, x, skin_vec=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the CBAM block.\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor of shape [B, C, H, W].\n",
    "        - skin_vec (torch.Tensor, optional): Skin tone vector, used only if `use_film=True`.\n",
    "\n",
    "        Returns:\n",
    "        - out (torch.Tensor): Attention-modulated output tensor.\n",
    "        \"\"\"\n",
    "        # === Debug: Check shapes early ===\n",
    "        self._check_input(x, skin_vec)\n",
    "\n",
    "        # === Channel Attention ===\n",
    "        ca = self.channel_attn(x)  # [B, C, 1, 1]\n",
    "        ca = ca * x  # Apply channel attention\n",
    "\n",
    "        # === Optional FiLM (Feature-wise Linear Modulation) ===\n",
    "        if self.use_film:\n",
    "            ca = self.film(ca, skin_vec)\n",
    "\n",
    "        # === Spatial Attention ===\n",
    "        sa = self.spatial_attention(ca)  # Apply spatial attention\n",
    "\n",
    "        out = sa * ca  # Apply spatial attention to the channel attention output\n",
    "        return out\n",
    "\n",
    "    def _check_input(self, x, skin_vec):\n",
    "        \"\"\"Helper method to check input tensor shapes and raise informative errors.\"\"\"\n",
    "        if x.dim() != 4:\n",
    "            raise ValueError(f\"‚ùå Expected 4D input [B, C, H, W], got {x.shape}\")\n",
    "        B, C, H, W = x.shape\n",
    "        if C != self.channels:\n",
    "            raise ValueError(f\"‚ùå Channel mismatch: got {C}, expected {self.channels}\")\n",
    "\n",
    "        if self.use_film:\n",
    "            if skin_vec is None:\n",
    "                raise ValueError(\"‚ùå use_film=True but skin_vec is None\")\n",
    "            if skin_vec.shape[0] != B:\n",
    "                raise ValueError(f\"‚ùå Batch mismatch: skin_vec batch {skin_vec.shape[0]} vs input {B}\")\n",
    "\n",
    "    def spatial_attention(self, x):\n",
    "        \"\"\"Apply spatial attention using max and average pooling.\"\"\"\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # [B, 1, H, W]\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # [B, 1, H, W]\n",
    "        sa_input = torch.cat([avg_out, max_out], dim=1)  # [B, 2, H, W]\n",
    "        sa = self.spatial_attn(sa_input)  # [B, 1, H, W]\n",
    "        return sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1763671113341,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "rwpO2wikttxy"
   },
   "outputs": [],
   "source": [
    "# @title FiLM\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, in_features, feature_map_channels):\n",
    "        \"\"\"\n",
    "        Feature-wise Linear Modulation (FiLM) Layer.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): Input size of conditioning vector (e.g., 12)\n",
    "            feature_map_channels (int): Number of channels in the input feature map\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Modify in_features to accept 12 instead of 10 for skin vector\n",
    "        self.gamma_fc = nn.Linear(in_features, feature_map_channels)\n",
    "        self.beta_fc = nn.Linear(in_features, feature_map_channels)\n",
    "\n",
    "        # Initialize gamma and beta\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize gamma and beta to specific values.\"\"\"\n",
    "        nn.init.constant_(self.gamma_fc.weight, 1)  # Initialize gamma weights to 1\n",
    "        nn.init.constant_(self.beta_fc.weight, 0)   # Initialize beta weights to 0\n",
    "        nn.init.constant_(self.gamma_fc.bias, 0)    # Initialize gamma bias to 0\n",
    "        nn.init.constant_(self.beta_fc.bias, 0)     # Initialize beta bias to 0\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Feature map tensor of shape [B, C, H, W]\n",
    "            cond (torch.Tensor): Conditioning vector (e.g., skin vector) of shape [B, in_features]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: FiLM-modulated feature map of shape [B, C, H, W]\n",
    "        \"\"\"\n",
    "        # Check dimensions\n",
    "        B, C, H, W = x.size()\n",
    "        assert cond.size(0) == B, f\"Batch size mismatch: x ({B}) vs cond ({cond.size(0)})\"\n",
    "        assert cond.size(1) == self.gamma_fc.in_features, \\\n",
    "            f\"Feature size mismatch: cond ({cond.size(1)}) vs in_features ({self.gamma_fc.in_features})\"\n",
    "\n",
    "        # Get the gamma and beta from the conditioning vector\n",
    "        gamma = self.gamma_fc(cond).view(B, C, 1, 1)  # [B, C, 1, 1]\n",
    "        beta = self.beta_fc(cond).view(B, C, 1, 1)    # [B, C, 1, 1]\n",
    "\n",
    "        # FiLM modulation: scale and shift the feature map\n",
    "        return gamma * x + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1763671113365,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "IIg8etvVrBSe"
   },
   "outputs": [],
   "source": [
    "# @title ResNet\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        backbone_name=\"resnet152d\",\n",
    "        attention_type=\"none\",\n",
    "        drop_path_rate=0.2,\n",
    "        dropout_rate=0.6,\n",
    "        use_film_before=False,\n",
    "        use_film_in_cbam=False,\n",
    "        use_triplet_embedding=False,\n",
    "        triplet_embedding_dim=512,\n",
    "        include_skin_vec=True,\n",
    "        fusion_mode=\"concat\",\n",
    "        fusion_hidden_dim=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_film_before = use_film_before\n",
    "        self.use_film_in_cbam = use_film_in_cbam\n",
    "        self.use_triplet_embedding = use_triplet_embedding\n",
    "        self.triplet_embedding_dim = triplet_embedding_dim\n",
    "        self.include_skin_vec = include_skin_vec\n",
    "        self.fusion_mode = fusion_mode\n",
    "        self.fusion_hidden_dim = fusion_hidden_dim\n",
    "\n",
    "        self.base = timm.create_model(backbone_name, pretrained=True, num_classes=0)\n",
    "        C = self.base.num_features\n",
    "        self._feat_dim = C\n",
    "\n",
    "        # ‚úÖ FiLM now uses 12-dim conditioning\n",
    "        if self.use_film_before:\n",
    "            self.film = FiLM(in_features=12, feature_map_channels=C)\n",
    "\n",
    "        # ‚úÖ CBAM now uses 12-dim cond\n",
    "        if attention_type == \"cbam\":\n",
    "            self.attn = CBAM(C, use_film=self.use_film_in_cbam, film_in_dim=12)\n",
    "        elif attention_type == \"self\":\n",
    "            self.attn = SelfAttentionBlock(C)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "        # ‚úÖ Skin MLP now 12-dim\n",
    "        self.skin_mlp = Skin_Multi_Layer_Perceptron(input_dim=12)\n",
    "\n",
    "        # Fusion + classifier logic same pattern as above if you're using fusion_mode\n",
    "        # (if you also have fusion here, apply the same \"final_in_dim then classifier\" pattern)\n",
    "\n",
    "\n",
    "        if fusion_mode in [\"mlp\", \"gated\"]:\n",
    "            self.image_proj = nn.Linear(C, fusion_hidden_dim)\n",
    "            self.skin_proj = nn.Linear(8, fusion_hidden_dim)\n",
    "            if self.use_triplet_embedding:\n",
    "                self.triplet_proj = nn.Linear(triplet_embedding_dim, fusion_hidden_dim)\n",
    "\n",
    "            if fusion_mode == \"gated\":\n",
    "                gate_input_dim = C + 8 + (triplet_embedding_dim if self.use_triplet_embedding else 0)\n",
    "                self.gate = nn.Sequential(\n",
    "                    nn.Linear(gate_input_dim, 3),\n",
    "                    nn.Softmax(dim=1)\n",
    "                )\n",
    "            final_in_dim = fusion_hidden_dim\n",
    "        else:\n",
    "            final_in_dim = C + 8 + (triplet_embedding_dim if self.use_triplet_embedding else 0)\n",
    "\n",
    "        self.expected_final_dim = final_in_dim\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.classifier = TwoLayerClassifierHead(input_dim=final_in_dim, output_dim=num_classes)\n",
    "\n",
    "    def forward_features(self, x, skin_vec):\n",
    "        x = self.base.forward_features(x)\n",
    "        if self.use_film_before:\n",
    "            x = self.film(x, skin_vec)\n",
    "        x = self.attn(x, skin_vec) if isinstance(self.attn, CBAM) else self.attn(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, skin_vec=None, triplet_embedding=None, return_features=False):\n",
    "        B = x.size(0)\n",
    "        if self.include_skin_vec and skin_vec is None:\n",
    "            skin_vec = torch.zeros((B, 10), device=x.device)\n",
    "\n",
    "        feat = self.forward_features(x, skin_vec)\n",
    "        feat = F.adaptive_avg_pool2d(feat, 1).view(B, -1)\n",
    "        features = feat\n",
    "\n",
    "        skin_feat = self.skin_mlp(skin_vec)\n",
    "\n",
    "        if self.use_triplet_embedding:\n",
    "            if triplet_embedding is None:\n",
    "                triplet_embedding = torch.zeros((B, self.triplet_embedding_dim), device=x.device, dtype=feat.dtype)\n",
    "\n",
    "        if self.fusion_mode == \"concat\":\n",
    "            parts = [feat, skin_feat] + ([triplet_embedding] if self.use_triplet_embedding else [])\n",
    "            final_feat = torch.cat(parts, dim=1)\n",
    "\n",
    "        elif self.fusion_mode == \"mlp\":\n",
    "            feat_proj = self.image_proj(feat)\n",
    "            skin_proj = self.skin_proj(skin_feat)\n",
    "            if self.use_triplet_embedding:\n",
    "                triplet_proj = self.triplet_proj(triplet_embedding)\n",
    "                final_feat = feat_proj + skin_proj + triplet_proj\n",
    "            else:\n",
    "                final_feat = feat_proj + skin_proj\n",
    "\n",
    "        elif self.fusion_mode == \"gated\":\n",
    "            gate_in = torch.cat([feat, skin_feat] + ([triplet_embedding] if self.use_triplet_embedding else []), dim=1)\n",
    "            weights = self.gate(gate_in)\n",
    "            feat_proj = self.image_proj(feat)\n",
    "            skin_proj = self.skin_proj(skin_feat)\n",
    "            if self.use_triplet_embedding:\n",
    "                triplet_proj = self.triplet_proj(triplet_embedding)\n",
    "                final_feat = weights[:, 0:1] * feat_proj + weights[:, 1:2] * skin_proj + weights[:, 2:3] * triplet_proj\n",
    "            else:\n",
    "                final_feat = weights[:, 0:1] * feat_proj + weights[:, 1:2] * skin_proj\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion mode: {self.fusion_mode}\")\n",
    "        #print(f\"Fusion: {self.fusion_mode}, Final feature shape: {final_feat.shape}\")\n",
    "        final_feat = self.dropout(final_feat)\n",
    "        logits = self.classifier(final_feat)\n",
    "\n",
    "        return (logits, features) if return_features else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763671113379,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "FGlNlkj7rM7T"
   },
   "outputs": [],
   "source": [
    "# @title EfficientNet\n",
    "\n",
    "class EfficientNetWithAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        attention_type=\"none\",\n",
    "        pretrained=True,\n",
    "        use_film=False,\n",
    "        use_film_before=False,\n",
    "        use_film_in_cbam=False,\n",
    "        use_triplet_embedding=False,\n",
    "        triplet_embedding_dim=512,\n",
    "        include_skin_vec=True,\n",
    "        efficientnet_variant=\"efficientnet_b0\",\n",
    "        dropout_rate=0.6,  # <-- This argument\n",
    "        fusion_mode=\"concat\",  # \"concat\", \"mlp\", or \"gated\"\n",
    "        fusion_hidden_dim=128,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = device if device is not None else torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.use_film_before = use_film_before\n",
    "        self.use_film_in_cbam = use_film_in_cbam\n",
    "        self.use_triplet_embedding = use_triplet_embedding\n",
    "        self.triplet_embedding_dim = triplet_embedding_dim\n",
    "        self.include_skin_vec = include_skin_vec\n",
    "        self.fusion_mode = fusion_mode\n",
    "        self.fusion_hidden_dim = fusion_hidden_dim\n",
    "\n",
    "        # Backbone\n",
    "        self.base = timm.create_model(\n",
    "            efficientnet_variant, pretrained=pretrained, num_classes=0\n",
    "        )\n",
    "        C = self.base.num_features\n",
    "        print(f\"Feature dimension (C): {C}\")\n",
    "        if not isinstance(C, int) or C <= 0:\n",
    "            raise ValueError(f\"Invalid feature dimension C: {C}\")\n",
    "        self._feat_dim = C\n",
    "\n",
    "        # FiLM before attention (skin_vec is 12-dim: ITA, hue, 10 MST one-hot)\n",
    "        if self.use_film_before:\n",
    "            self.film = FiLM(in_features=12, feature_map_channels=C)\n",
    "        else:\n",
    "            self.film = nn.Identity() # Added for consistency\n",
    "\n",
    "        # Attention\n",
    "        if attention_type == \"self\":\n",
    "            self.attn = SelfAttentionBlock(C)\n",
    "        elif attention_type == \"cbam\":\n",
    "            self.attn = CBAM(C, use_film=self.use_film_in_cbam, film_in_dim=12)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "        # Skin MLP: 12 -> 8\n",
    "        self.skin_mlp = Skin_Multi_Layer_Perceptron(input_dim=12)  # -> 8D\n",
    "\n",
    "        # üîë FUSION BLOCK ‚Äî this decides final_in_dim\n",
    "        if self.fusion_mode in [\"mlp\", \"gated\"]:\n",
    "            print(f\"Fusion Dim: {fusion_hidden_dim}\")\n",
    "            if not isinstance(fusion_hidden_dim, int) or fusion_hidden_dim <= 0:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid fusion_hidden_dim: {fusion_hidden_dim}. It should be a positive integer.\"\n",
    "                )\n",
    "\n",
    "            self.image_proj = nn.Linear(C, fusion_hidden_dim).to(\n",
    "                self.device, dtype=torch.float32\n",
    "            )\n",
    "            self.skin_proj = nn.Linear(8, fusion_hidden_dim).to(\n",
    "                self.device, dtype=torch.float32\n",
    "            )\n",
    "            if self.use_triplet_embedding:\n",
    "                self.triplet_proj = nn.Linear(\n",
    "                    triplet_embedding_dim, fusion_hidden_dim\n",
    "                ).to(self.device, dtype=torch.float32)\n",
    "\n",
    "            if self.fusion_mode == \"gated\":\n",
    "                gate_input_dim = (\n",
    "                    C\n",
    "                    + 8\n",
    "                    + (triplet_embedding_dim if self.use_triplet_embedding else 0)\n",
    "                )\n",
    "                self.gate = nn.Sequential(\n",
    "                    nn.Linear(gate_input_dim, 3),\n",
    "                    nn.Softmax(dim=1),\n",
    "                ).to(self.device, dtype=torch.float32)\n",
    "\n",
    "            final_in_dim = fusion_hidden_dim\n",
    "        else:\n",
    "            # \"concat\" mode\n",
    "            final_in_dim = C + 8 + (triplet_embedding_dim if use_triplet_embedding else 0)\n",
    "\n",
    "        self.final_in_dim = final_in_dim\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_rate) # Now uses the argument\n",
    "\n",
    "        #CLASSIFIER CREATED AFTER fusion_dim is known\n",
    "        self.classifier = TwoLayerClassifierHead(\n",
    "            input_dim=self.final_in_dim, output_dim=num_classes\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x, skin_vec):\n",
    "        x = self.base.forward_features(x)\n",
    "        # Apply film if it exists\n",
    "        if hasattr(self, 'film'):\n",
    "             x = self.film(x, skin_vec)\n",
    "        x = self.attn(x, skin_vec) if isinstance(self.attn, CBAM) else self.attn(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, skin_vec=None, triplet_embedding=None, return_features=False):\n",
    "        B = x.size(0)\n",
    "        if self.include_skin_vec and skin_vec is None:\n",
    "            skin_vec = torch.zeros((B, 12), device=x.device, dtype=x.dtype) # Use dtype of x\n",
    "\n",
    "        feat = self.forward_features(x, skin_vec)\n",
    "        feat = F.adaptive_avg_pool2d(feat, 1).view(B, -1)\n",
    "\n",
    "        skin_feat = self.skin_mlp(skin_vec)\n",
    "\n",
    "        if self.use_triplet_embedding:\n",
    "            if triplet_embedding is None:\n",
    "                triplet_embedding = torch.zeros(\n",
    "                    (B, self.triplet_embedding_dim), device=x.device, dtype=feat.dtype # Use dtype of feat\n",
    "                )\n",
    "        else:\n",
    "            triplet_embedding = None\n",
    "\n",
    "        # Fusion logic based on fusion_mode\n",
    "        if self.fusion_mode == \"concat\":\n",
    "            parts = [feat, skin_feat] + (\n",
    "                [triplet_embedding] if self.use_triplet_embedding else []\n",
    "            )\n",
    "            final_feat = torch.cat(parts, dim=1)\n",
    "        elif self.fusion_mode == \"mlp\":\n",
    "            feat_proj = self.image_proj(feat)\n",
    "            skin_proj = self.skin_proj(skin_feat)\n",
    "            if self.use_triplet_embedding:\n",
    "                triplet_proj = self.triplet_proj(triplet_embedding)\n",
    "                final_feat = feat_proj + skin_proj + triplet_proj\n",
    "            else:\n",
    "                final_feat = feat_proj + skin_proj\n",
    "        elif self.fusion_mode == \"gated\":\n",
    "            gate_in = torch.cat(\n",
    "                [feat, skin_feat]\n",
    "                + ([triplet_embedding] if self.use_triplet_embedding else []),\n",
    "                dim=1,\n",
    "            )\n",
    "            weights = self.gate(gate_in)\n",
    "            feat_proj = self.image_proj(feat)\n",
    "            skin_proj = self.skin_proj(skin_feat)\n",
    "            if self.use_triplet_embedding:\n",
    "                triplet_proj = self.triplet_proj(triplet_embedding)\n",
    "                final_feat = (\n",
    "                    weights[:, 0:1] * feat_proj\n",
    "                    + weights[:, 1:2] * skin_proj\n",
    "                    + weights[:, 2:3] * triplet_proj\n",
    "                )\n",
    "            else:\n",
    "                final_feat = (\n",
    "                    weights[:, 0:1] * feat_proj + weights[:, 1:2] * skin_proj\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion mode: {self.fusion_mode}\")\n",
    "        final_feat = self.dropout(final_feat)\n",
    "        logits = self.classifier(final_feat)\n",
    "\n",
    "        return (logits, feat) if return_features else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1763671113381,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "_KirO4JNretM"
   },
   "outputs": [],
   "source": [
    "# @title InceptionNet V3\n",
    "\n",
    "class InceptionV3WithAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        attention_type=\"none\",\n",
    "        pretrained=True,\n",
    "        use_film_before=False,\n",
    "        use_film_in_cbam=False,\n",
    "        use_triplet_embedding=False,\n",
    "        triplet_embedding_dim=512,\n",
    "        include_skin_vec=True,\n",
    "        fusion_mode=\"concat\",   # \"concat\", \"mlp\", \"gated\"\n",
    "        fusion_hidden_dim=128,\n",
    "        dropout_rate=0.8,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.include_skin_vec = include_skin_vec\n",
    "        self.use_triplet_embedding = use_triplet_embedding\n",
    "        self.fusion_mode = fusion_mode\n",
    "        self.fusion_hidden_dim = fusion_hidden_dim\n",
    "\n",
    "        # Backbone\n",
    "        self.base = timm.create_model(\"inception_v3\", pretrained=pretrained, num_classes=0)\n",
    "        feature_dim = self.base.num_features\n",
    "\n",
    "        # FiLM\n",
    "        self.film = FiLM(in_features=12, feature_map_channels=feature_dim) if use_film_before else nn.Identity()\n",
    "\n",
    "        # Attention\n",
    "        if attention_type == \"cbam\":\n",
    "            self.attn = CBAM(channels=feature_dim, use_film=use_film_in_cbam, film_in_dim=12)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "        # Skin MLP\n",
    "        self.skin_mlp = Skin_Multi_Layer_Perceptron(input_dim=12)\n",
    "\n",
    "        # üîë Fusion logic\n",
    "        if self.fusion_mode in [\"mlp\", \"gated\"]:\n",
    "            if not isinstance(fusion_hidden_dim, int) or fusion_hidden_dim <= 0:\n",
    "                raise ValueError(f\"Invalid fusion_hidden_dim: {fusion_hidden_dim}\")\n",
    "\n",
    "            self.image_proj = nn.Linear(feature_dim, fusion_hidden_dim)\n",
    "            self.skin_proj = nn.Linear(8, fusion_hidden_dim)\n",
    "            if self.use_triplet_embedding:\n",
    "                self.triplet_proj = nn.Linear(triplet_embedding_dim, fusion_hidden_dim)\n",
    "\n",
    "            if self.fusion_mode == \"gated\":\n",
    "                gate_input_dim = (\n",
    "                    feature_dim\n",
    "                    + 8\n",
    "                    + (triplet_embedding_dim if self.use_triplet_embedding else 0)\n",
    "                )\n",
    "                self.gate = nn.Sequential(\n",
    "                    nn.Linear(gate_input_dim, 3),\n",
    "                    nn.Softmax(dim=1),\n",
    "                )\n",
    "            final_in_dim = fusion_hidden_dim\n",
    "        else:\n",
    "            # concat\n",
    "            final_in_dim = feature_dim + 8 + (triplet_embedding_dim if self.use_triplet_embedding else 0)\n",
    "\n",
    "        self.final_in_dim = final_in_dim\n",
    "\n",
    "        # Was: self.dropout = nn.Dropout(p=0.3)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate) # Now uses the argument\n",
    "\n",
    "\n",
    "        # ‚úÖ Classifier created using final_in_dim\n",
    "        self.classifier = TwoLayerClassifierHead(\n",
    "            input_dim=self.final_in_dim, output_dim=num_classes\n",
    "        )\n",
    "\n",
    "    def get_gradcam_target_layer(self):\n",
    "        return self.base.Mixed_7c\n",
    "\n",
    "    def forward_features(self, x, skin_vec):\n",
    "        \"\"\"Extract features from the image with optional FiLM and attention.\"\"\"\n",
    "        x = self.base.forward_features(x)  # Extract features from the base model\n",
    "        x = self.film(x, skin_vec)\n",
    "        x = self.attn(x, skin_vec) if isinstance(self.attn, CBAM) else self.attn(x) # Handle Identity\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, skin_vec=None, triplet_embedding=None, return_features=False):\n",
    "        \"\"\"Forward pass through the model, including feature extraction, fusion, and classification.\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        if self.include_skin_vec and skin_vec is None:\n",
    "            skin_vec = torch.zeros((batch_size, 12), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        feat = self.forward_features(x, skin_vec)\n",
    "        feat = F.adaptive_avg_pool2d(feat, 1).view(batch_size, -1)\n",
    "\n",
    "        skin_feat = self.skin_mlp(skin_vec)\n",
    "\n",
    "        if self.use_triplet_embedding:\n",
    "            if triplet_embedding is None:\n",
    "                triplet_embedding = torch.zeros((batch_size, 512), device=x.device, dtype=feat.dtype) # Use feat dtype\n",
    "\n",
    "        # Fusion: Concatenate or use MLP/Gated fusion\n",
    "        if self.fusion_mode == \"concat\":\n",
    "            parts = [feat, skin_feat] + ([triplet_embedding] if self.use_triplet_embedding else [])\n",
    "            final_feat = torch.cat(parts, dim=1)\n",
    "        elif self.fusion_mode == \"mlp\":\n",
    "            feat_proj = self.image_proj(feat)\n",
    "            skin_proj = self.skin_proj(skin_feat)\n",
    "            if self.use_triplet_embedding:\n",
    "                triplet_proj = self.triplet_proj(triplet_embedding)\n",
    "                final_feat = feat_proj + skin_proj + triplet_proj\n",
    "            else:\n",
    "                final_feat = feat_proj + skin_proj\n",
    "        elif self.fusion_mode == \"gated\":\n",
    "            gate_in = torch.cat([feat, skin_feat] + ([triplet_embedding] if self.use_triplet_embedding else []), dim=1)\n",
    "            weights = self.gate(gate_in)\n",
    "            feat_proj = self.image_proj(feat)\n",
    "            skin_proj = self.skin_proj(skin_feat)\n",
    "            if self.use_triplet_embedding:\n",
    "                triplet_proj = self.triplet_proj(triplet_embedding)\n",
    "                final_feat = weights[:, 0:1] * feat_proj + weights[:, 1:2] * skin_proj + weights[:, 2:3] * triplet_proj\n",
    "            else:\n",
    "                final_feat = weights[:, 0:1] * feat_proj + weights[:, 1:2] * skin_proj\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion mode: {self.fusion_mode}\")\n",
    "        #print(f\"Fusion: {self.fusion_mode}, Final feature shape: {final_feat.shape}\")\n",
    "        final_feat = self.dropout(final_feat)\n",
    "        logits = self.classifier(final_feat)\n",
    "\n",
    "        return (logits, feat) if return_features else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1763671113400,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "yxpZAuD7rxxE"
   },
   "outputs": [],
   "source": [
    "# @title Plotting Functions\n",
    "\n",
    "# Set up logging for error handling and info messages\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "# Ensure the directory exists\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def tensor_to_bgr_uint8(img: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a CHW tensor in [0,1] or roughly normalized to a BGR uint8 image.\n",
    "    If it was normalized with ImageNet stats, this still looks acceptable after min-max.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = img.detach().float().cpu()\n",
    "\n",
    "        # If it is a 4D tensor, take the first element (batch size)\n",
    "        if x.ndimension() == 4:\n",
    "            x = x[0]\n",
    "\n",
    "        # Min-max to [0, 1] and convert to [0, 255] BGR\n",
    "        x -= x.min()\n",
    "        denom = (x.max() - x.min()).clamp(min=1e-6)\n",
    "        x = x / denom\n",
    "        x = (x * 255.0).clamp(0, 255).byte()\n",
    "\n",
    "        # Convert CHW to HWC and then BGR (from RGB)\n",
    "        x = x.numpy().transpose(1, 2, 0)\n",
    "        return x[..., ::-1]  # Convert RGB to BGR\n",
    "\n",
    "\n",
    "def plot_tsne(features, labels, label_encoder, graph_dir, model_name, fold_classes, attention_type=\"CBAM\", save_plots=False):\n",
    "    \"\"\"Plot t-SNE visualization.\"\"\"\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='jet', s=50)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f\"t-SNE visualization for {model_name}\")\n",
    "    if save_plots:\n",
    "        plot_path = os.path.join(graph_dir, f\"tsne_{model_name}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"‚úÖ t-SNE plot saved to: {plot_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def overlay_heatmap(heatmap: np.ndarray, image_path: str, alpha: float = 0.5, colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Overlays a heatmap onto an image for visualization.\n",
    "\n",
    "    Args:\n",
    "        heatmap (np.ndarray): The heatmap array (grayscale, 0-1).\n",
    "        image_path (str): The path to the original image.\n",
    "        alpha (float): The transparency of the heatmap overlay.\n",
    "        colormap (int): The OpenCV colormap to apply to the heatmap.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The original image with the heatmap overlayed.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Read and resize the original image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Could not read image at {image_path}\")\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        # Resize the heatmap and apply the colormap\n",
    "        heatmap_resized = cv2.resize(heatmap, (224, 224))\n",
    "        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), colormap)\n",
    "\n",
    "        # Blend the image and the heatmap\n",
    "        overlayed_image = cv2.addWeighted(heatmap_colored, alpha, image, 1 - alpha, 0)\n",
    "\n",
    "        return overlayed_image\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in overlay_heatmap: {e}\")\n",
    "        # Return the original image if an error occurs\n",
    "        return cv2.imread(image_path)\n",
    "\n",
    "\n",
    "# --- Class and Skin Group Count Plot (Interactive Version) ---\n",
    "def plot_class_skin_group_counts_interactive(y, z, group_fn=bin_mst_to_skin_group, save_path=None):\n",
    "    \"\"\"Generates an interactive heatmap showing class distribution across skin groups.\"\"\"\n",
    "    combo_counts = Counter()\n",
    "    for label, skin_vec in zip(y, z):\n",
    "        if not isinstance(skin_vec, dict) or \"MST\" not in skin_vec:\n",
    "            continue\n",
    "        group = group_fn(skin_vec[\"MST\"])\n",
    "        combo_counts[(label, group)] += 1\n",
    "\n",
    "    df = pd.DataFrame([{\"Class\": k[0], \"Skin_Group\": k[1], \"Count\": v} for k, v in combo_counts.items()])\n",
    "    pivot = df.pivot(index=\"Class\", columns=\"Skin_Group\", values=\"Count\").fillna(0)\n",
    "\n",
    "    fig = px.imshow(pivot, labels=dict(x=\"Skin Group\", y=\"Class\", color=\"Count\"),\n",
    "                    color_continuous_scale='Viridis', title=\"Samples per (Class, Skin Group)\")\n",
    "    fig.update_layout(title=\"Samples per (Class, Skin Group)\", autosize=True)\n",
    "\n",
    "    if save_path:\n",
    "        fig.write_html(save_path)\n",
    "        logging.info(f\"Interactive class-group heatmap saved to: {save_path}\")\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "# --- MST Distribution Plot ---\n",
    "def plot_mst_distribution_by_class(y_true, mst_bins, class_names, save_path=None):\n",
    "    \"\"\"Plots MST distribution by class in a stacked bar chart.\"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame({\n",
    "            'Class': pd.Categorical([class_names[int(y)] for y in y_true], categories=class_names, ordered=True),\n",
    "            'MST Bin': mst_bins\n",
    "        })\n",
    "        crosstab = pd.crosstab(df['Class'], df['MST Bin'], dropna=False)\n",
    "        crosstab_pct = crosstab.div(crosstab.sum(axis=1), axis=0) * 100\n",
    "        ax = crosstab_pct.plot(\n",
    "            kind='bar', stacked=True, figsize=(18, 10),\n",
    "            colormap='viridis', width=0.8\n",
    "        )\n",
    "        ax.set_title(\"MST Distribution by Class\", fontsize=20, pad=20)\n",
    "        ax.set_xlabel(\"Class\", fontsize=14)\n",
    "        ax.set_ylabel(\"Percentage\", fontsize=14)\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "        ax.legend(title=\"MST Bin\", bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=12)\n",
    "        for container in ax.containers:\n",
    "            labels = [f\"{w:.1f}%\" if w > 4 else \"\" for w in container.datavalues]\n",
    "            ax.bar_label(container, labels=labels, label_type='center', color='white', weight='bold', fontsize=10)\n",
    "        plt.tight_layout(rect=[0, 0, 0.88, 1])\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logging.info(f\"Saved MST distribution plot to: {save_path}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate MST distribution plot: {e}\")\n",
    "\n",
    "\n",
    "# --- Training Curves Plot ---\n",
    "def plot_training_curves(history, save_dir, model_name=\"\"):\n",
    "    \"\"\"Plots training and validation curves (loss/accuracy).\"\"\"\n",
    "    try:\n",
    "        prefix = f\"{model_name}_\" if model_name else \"\"\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epochs, history['train_loss'], 'o-', label=\"Train Loss\")\n",
    "        plt.plot(epochs, history['val_loss'], 'o-', label=\"Val Loss\")\n",
    "        plt.title(f\"{model_name} - Training & Validation Loss\")\n",
    "        plt.legend(); plt.grid(True)\n",
    "        plt.savefig(os.path.join(save_dir, f\"{prefix}loss_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Accuracy Plot\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epochs, history['train_acc'], 'o-', label=\"Train Accuracy\")\n",
    "        plt.plot(epochs, history['val_acc'], 'o-', label=\"Val Accuracy\")\n",
    "        plt.title(f\"{model_name} - Training & Validation Accuracy\")\n",
    "        plt.legend(); plt.grid(True)\n",
    "        plt.savefig(os.path.join(save_dir, f\"{prefix}accuracy_curve.png\"))\n",
    "        plt.close()\n",
    "        logging.info(f\"Training curves saved for: {model_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate training curves: {e}\")\n",
    "\n",
    "\n",
    "def plot_evaluation_results(\n",
    "    model_name: str,\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_probs: np.ndarray,\n",
    "    confusion: np.ndarray,\n",
    "    class_names: List[str],\n",
    "    skin_vecs: List[Dict],\n",
    "    mst_bins: List,\n",
    "    skin_groups: List[str],\n",
    "    output_dir: str,\n",
    "    save_training_curves: bool = False,\n",
    "    training_curves_data: Optional[Dict] = None,\n",
    "    features: Optional[np.ndarray] = None\n",
    ") -> None:\n",
    "    \"\"\"A master function to call all individual plotting utilities.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    try:\n",
    "        cm_path = os.path.join(output_dir, f'{model_name}_confusion.png')\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel(\"Predicted Label\"); plt.ylabel(\"True Label\")\n",
    "        plt.title(f\"{model_name} Confusion Matrix\")\n",
    "        plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logging.info(f\"Confusion matrix saved to: {cm_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Confusion matrix plot failed: {e}\")\n",
    "\n",
    "    # --- Fairness Plot ---\n",
    "    try:\n",
    "        fairness_df = compute_fairness_by_group(y_true, y_probs, class_names, skin_groups=skin_groups)\n",
    "        fairness_plot_path = os.path.join(output_dir, f\"{model_name}_fairness.png\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.scatterplot(data=fairness_df, x=\"Accuracy\", y=\"F1\", hue=\"Skin Group\", s=100)\n",
    "        plt.title(\"Accuracy vs F1 Score by Skin Group\")\n",
    "        plt.xlim(0, 1); plt.ylim(0, 1); plt.grid(True)\n",
    "        plt.savefig(fairness_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logging.info(f\"Fairness plot saved to: {fairness_plot_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fairness plot failed: {e}\")\n",
    "\n",
    "    # --- MST Distribution ---\n",
    "    mst_dist_path = os.path.join(output_dir, f\"{model_name}_mst_distribution.png\")\n",
    "    plot_mst_distribution_by_class(y_true, mst_bins, class_names, save_path=mst_dist_path)\n",
    "\n",
    "    # --- Training Curves ---\n",
    "    if save_training_curves and training_curves_data:\n",
    "        plot_training_curves(\n",
    "            history=training_curves_data,\n",
    "            save_dir=output_dir,\n",
    "            model_name=model_name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1763671113403,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "zLWs1k3NvsdP"
   },
   "outputs": [],
   "source": [
    "# @title Triplet Loss\n",
    "\n",
    "class InBatchHardTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes triplet loss using hard positive and hard negative mining within the batch.\n",
    "    Assumes anchor, positive, and negative are all drawn from the model's learned features (feat).\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.0, reduction='mean'):\n",
    "        super(InBatchHardTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (torch.Tensor): Tensor of shape (batch_size, embedding_dim)\n",
    "                                     representing the model's learned features (feat).\n",
    "            labels (torch.Tensor): Tensor of shape (batch_size,) representing the class labels.\n",
    "        Returns:\n",
    "            torch.Tensor: Scalar triplet loss.\n",
    "        \"\"\"\n",
    "        if features.size(0) < 2:\n",
    "            # Not enough samples in the batch to form triplets, return 0 loss\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "\n",
    "        # Calculate pairwise Euclidean distances\n",
    "        # (a-b)^2 = a^2 + b^2 - 2ab\n",
    "        dot_product = torch.matmul(features, features.transpose(0, 1))\n",
    "        square_norm = torch.diag(dot_product) # Sum of squares of each vector\n",
    "        distances = square_norm.unsqueeze(1) + square_norm.unsqueeze(0) - 2 * dot_product\n",
    "        distances = torch.sqrt(F.relu(distances) + 1e-16) # Add epsilon for numerical stability\n",
    "\n",
    "        # Initialize loss\n",
    "        triplet_loss = torch.tensor(0.0, device=features.device)\n",
    "        num_valid_triplets = 0\n",
    "\n",
    "        for i in range(features.size(0)):\n",
    "            anchor_feature = features[i]\n",
    "            anchor_label = labels[i]\n",
    "\n",
    "            # Find positive samples (same class as anchor, excluding anchor itself)\n",
    "            positive_mask = (labels == anchor_label) & (torch.arange(features.size(0), device=features.device) != i)\n",
    "            positive_distances = distances[i][positive_mask]\n",
    "\n",
    "            # Find negative samples (different class from anchor)\n",
    "            negative_mask = (labels != anchor_label)\n",
    "            negative_distances = distances[i][negative_mask]\n",
    "\n",
    "            if positive_distances.numel() > 0 and negative_distances.numel() > 0:\n",
    "                # Hard positive mining: pick the farthest positive from anchor\n",
    "                hard_positive_dist = torch.max(positive_distances)\n",
    "                # Hard negative mining: pick the closest negative to anchor\n",
    "                hard_negative_dist = torch.min(negative_distances)\n",
    "\n",
    "                # Compute triplet loss for this anchor\n",
    "                loss_i = F.relu(hard_positive_dist - hard_negative_dist + self.margin)\n",
    "                if loss_i > 0: # Only accumulate if the loss is positive (i.e., violation)\n",
    "                    triplet_loss += loss_i\n",
    "                    num_valid_triplets += 1\n",
    "\n",
    "        if num_valid_triplets == 0:\n",
    "            # If no valid triplets could be formed in the batch, return 0 loss\n",
    "            return torch.tensor(0.0, device=features.device, requires_grad=True)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return triplet_loss / num_valid_triplets\n",
    "        elif self.reduction == 'sum':\n",
    "            return triplet_loss\n",
    "        else: # reduction == 'none' or other\n",
    "            return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1763671113426,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "JUsEAGJ1se1_"
   },
   "outputs": [],
   "source": [
    "# @title Evaluation Function\n",
    "\n",
    "def evaluate_model(\n",
    "    model, test_loader, device, label_encoder,\n",
    "    save_dir, model_name=\"model\", graph_dir=None,\n",
    "    save_training_curves=False, training_curves_data=None, fold_classes=None,\n",
    "    gradcam_layer=None, visualize_gradcam=False, max_gradcam_images=3,\n",
    "    use_amp=False, plot_tsne_enabled=False  # Add plot_tsne_enabled flag\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and generates various evaluation metrics including accuracy,\n",
    "    classification report, confusion matrix, and optional t-SNE plots.\n",
    "    \"\"\"\n",
    "    ensure_dir(save_dir)\n",
    "    if graph_dir:\n",
    "        ensure_dir(graph_dir)\n",
    "\n",
    "    model.eval()\n",
    "    clear_all_forward_hooks(model)\n",
    "\n",
    "    y_true, y_pred, y_probs = [], [], []\n",
    "    all_mst_bins, all_skin_groups, all_raw_metadata = [], [], []\n",
    "\n",
    "    print(f\"\\n--- Evaluating model: {model_name} ---\")\n",
    "\n",
    "    # Plain eval\n",
    "    with torch.no_grad():\n",
    "        features_list, labels_list = [], []  # Collect features for t-SNE\n",
    "\n",
    "        for batch in test_loader:\n",
    "            # Correctly unpack all 7 items returned by CustomDataset\n",
    "            inputs, labels, skin_vecs, triplet_embeddings, mst_bins_batch, skin_groups_batch, raw_meta_batch = batch\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            skin_vecs = skin_vecs.to(device)\n",
    "\n",
    "            # Triplet embeddings should already be a tensor from CustomDataset\n",
    "            triplet_embeddings = triplet_embeddings.to(device) if triplet_embeddings is not None else None\n",
    "\n",
    "            # --- MODIFICATION: Pass return_features=True to get the actual features ---\n",
    "            outputs, features_for_tsne = model(inputs, skin_vecs, triplet_embedding=triplet_embeddings, return_features=True)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = probs.argmax(dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_probs.extend(probs.cpu().numpy())\n",
    "            all_mst_bins.extend(mst_bins_batch.numpy() if hasattr(mst_bins_batch, \"numpy\") else np.asarray(mst_bins_batch))\n",
    "            all_skin_groups.extend(skin_groups_batch)\n",
    "            all_raw_metadata.append(raw_meta_batch)  # Keep as-is for later use\n",
    "\n",
    "            # Collect features for t-SNE (before classification)\n",
    "            features_list.append(features_for_tsne.cpu().numpy()) # --- MODIFICATION: Use extracted features ---\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "    # Combine features and labels for t-SNE\n",
    "    features = np.concatenate(features_list, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    # Optional t-SNE plot\n",
    "    if plot_tsne_enabled:\n",
    "        plot_tsne(features, labels, label_encoder, graph_dir, model_name, fold_classes, attention_type=\"CBAM\", save_plots=True)\n",
    "\n",
    "    # Metrics/report\n",
    "    y_true = np.array(y_true); y_pred = np.array(y_pred); y_probs = np.array(y_probs)\n",
    "    cls_names = [label_encoder.inverse_transform([cls])[0] for cls in sorted(np.unique(y_true))]\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=cls_names, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=sorted(np.unique(y_true)))\n",
    "\n",
    "    print(f\"\\nAccuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    report_path = os.path.join(save_dir, f\"{model_name}_report.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(f\"Accuracy: {acc * 100:.2f}%\\n\\n{report}\\n\\nConfusion Matrix:\\n{np.array2string(cm)}\")\n",
    "    print(f\"‚úÖ Evaluation report saved to: {report_path}\")\n",
    "\n",
    "    # Plotting entrypoint\n",
    "    if graph_dir:\n",
    "        try:\n",
    "            plot_evaluation_results(\n",
    "                model_name=model_name,\n",
    "                y_true=y_true, y_pred=y_pred, y_probs=y_probs, confusion=cm,\n",
    "                class_names=cls_names, skin_vecs=all_raw_metadata,\n",
    "                mst_bins=all_mst_bins, skin_groups=all_skin_groups,\n",
    "                output_dir=graph_dir,\n",
    "                save_training_curves=save_training_curves,\n",
    "                training_curves_data=training_curves_data\n",
    "            )\n",
    "            print(f\"üìä Plots saved under: {graph_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Plotting failed: {e}\")\n",
    "\n",
    "    return acc, report, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1763671113429,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "Yjixrn9hudJM"
   },
   "outputs": [],
   "source": [
    "# @title Utilities\n",
    "\n",
    "def get_output_channels(model_name):\n",
    "    if model_name == \"resnet18\":\n",
    "        return 512\n",
    "    elif model_name in [\"resnet50v2\", \"resnet101v2\", \"resnet101d\", \"resnet152d\", \"resnetrs101\"]:\n",
    "        return 2048\n",
    "    elif model_name == \"inception_v3\": # <-- ADDED\n",
    "        return 2048                  # <-- ADDED\n",
    "    elif model_name in [\"mobilenet_v2\", \"googlenet\"]:\n",
    "        return 1280\n",
    "    elif model_name.startswith(\"vgg\"):\n",
    "        return 4096\n",
    "    elif model_name == \"alexnet\":\n",
    "        return 4096\n",
    "    elif model_name == \"densenet201\":\n",
    "        return 1920\n",
    "    elif model_name.startswith(\"efficientnet_b\"):\n",
    "        tf_variant_map = {\n",
    "            \"efficientnet_b4\": \"tf_efficientnet_b4_ns\",\n",
    "            \"efficientnet_b5\": \"tf_efficientnet_b5_ns\",\n",
    "            \"efficientnet_b6\": \"tf_efficientnet_b6_ns\",\n",
    "            \"efficientnet_b7\": \"tf_efficientnet_b7_ns\",\n",
    "        }\n",
    "        tf_variant = tf_variant_map.get(model_name, model_name)\n",
    "        backbone = timm.create_model(tf_variant, pretrained=True, num_classes=0)\n",
    "        return backbone.num_features\n",
    "    else:\n",
    "        # Try loading with timm dynamically\n",
    "        try:\n",
    "            backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "            return backbone.num_features\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "\n",
    "def compute_classwise_alpha(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    num_classes=4,\n",
    "    normalize=True,\n",
    "    clip_range=(0.1, 3.0),\n",
    "    prev_alpha=None,\n",
    "    beta=0.9,\n",
    "    smoothing=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute smoothed, capped alpha weights for Focal Loss based on inverse recall.\n",
    "\n",
    "    Args:\n",
    "        y_true (array): Ground truth labels.\n",
    "        y_pred (array): Predicted labels.\n",
    "        num_classes (int): Number of classes.\n",
    "        normalize (bool): Whether to normalize alpha to sum to num_classes.\n",
    "        clip_range (tuple): Min and max values to clip alpha.\n",
    "        prev_alpha (np.ndarray or torch.Tensor): Previous epoch's alpha for smoothing.\n",
    "        beta (float): Smoothing factor for EMA.\n",
    "        smoothing (bool): Whether to apply exponential smoothing.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Alpha weights.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    recalls = cm.diagonal() / (cm.sum(axis=1) + 1e-6)  # Avoid division by zero\n",
    "    alphas = 1.0 / (recalls + 1e-6)\n",
    "\n",
    "    # Clip alpha to avoid extreme weights\n",
    "    alphas = np.clip(alphas, clip_range[0], clip_range[1])\n",
    "\n",
    "    # Smooth with EMA using previous alpha\n",
    "    if smoothing and prev_alpha is not None:\n",
    "        if isinstance(prev_alpha, torch.Tensor):\n",
    "            prev_alpha = prev_alpha.detach().cpu().numpy()\n",
    "        alphas = beta * prev_alpha + (1 - beta) * alphas\n",
    "\n",
    "\n",
    "    # Normalize to keep total scale constant\n",
    "    if normalize:\n",
    "        alphas = alphas / alphas.sum() * num_classes\n",
    "\n",
    "    print(\"üîç Dynamic Alpha (inverse recall):\", np.round(alphas, 4))\n",
    "    return torch.tensor(alphas, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class GradualUnfreezer:\n",
    "    \"\"\"\n",
    "    Gradually unfreezes backbone layers from the end towards the beginning (high-level to low-level features).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, base_lr=0.001, start_epoch=5, unfreeze_every=5, max_blocks=None, weight_decay=1e-4):\n",
    "        self.model = model\n",
    "        self.base_lr = base_lr\n",
    "        self.unfreeze_every = unfreeze_every\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Freeze all backbone parameters initially\n",
    "        for p in self.model.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Break the backbone into its main sequential child modules\n",
    "        self.children = list(model.base.children())\n",
    "        self.total_blocks = len(self.children)\n",
    "        self.max_blocks_to_unfreeze = max_blocks if max_blocks is not None else self.total_blocks\n",
    "\n",
    "        # Start the pointer at the LAST block for backward unfreezing\n",
    "        self.next_block_to_unfreeze = self.total_blocks - 1\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.next_unfreeze_epoch = self.start_epoch\n",
    "\n",
    "        print(f\"üßä Backbone frozen: {self.total_blocks} total blocks.\")\n",
    "        print(f\"üìÖ Strategy: Unfreeze from the last block backward (high-level features first).\")\n",
    "        print(f\"   - Starting from block #{self.next_block_to_unfreeze} at epoch {self.start_epoch}.\")\n",
    "        print(f\"   - Unfreezing one block every {self.unfreeze_every} epoch(s).\")\n",
    "        print(f\"   - A maximum of {self.max_blocks_to_unfreeze} blocks will be unfrozen.\")\n",
    "\n",
    "    def step(self, optimizer: Optimizer, current_epoch: int):\n",
    "        \"\"\"\n",
    "        This method should be called at the beginning of each training epoch.\n",
    "        It now correctly uses 'self.next_block_to_unfreeze' for backward unfreezing.\n",
    "        \"\"\"\n",
    "        # 1. Check if it's the right time to unfreeze\n",
    "        if current_epoch < self.start_epoch or current_epoch < self.next_unfreeze_epoch:\n",
    "            return\n",
    "\n",
    "        # 2. Check if we have already unfrozen all possible blocks\n",
    "        if self.next_block_to_unfreeze < 0:\n",
    "            return\n",
    "\n",
    "        # 3. Check if we have reached the user-defined limit of blocks to unfreeze\n",
    "        unfrozen_count = (self.total_blocks - 1) - self.next_block_to_unfreeze\n",
    "        if unfrozen_count >= self.max_blocks_to_unfreeze:\n",
    "            return\n",
    "\n",
    "        # 4. Get the block to unfreeze\n",
    "        block_to_unfreeze = self.children[self.next_block_to_unfreeze]\n",
    "\n",
    "        # 5. Find parameters in this block that are not already in the optimizer\n",
    "        param_ids_in_optimizer = {id(p) for group in optimizer.param_groups for p in group['params']}\n",
    "        new_params = [p for p in block_to_unfreeze.parameters() if id(p) not in param_ids_in_optimizer]\n",
    "\n",
    "        if not new_params:\n",
    "            print(f\"Epoch {current_epoch}: Block {self.next_block_to_unfreeze} params are already in optimizer. Skipping.\")\n",
    "        else:\n",
    "            print(f\"üî• Epoch {current_epoch}: Unfreezing backbone block {self.next_block_to_unfreeze}...\")\n",
    "            for param in new_params:\n",
    "                param.requires_grad = True\n",
    "\n",
    "            # Add the newly trainable parameters to the optimizer with a smaller learning rate\n",
    "            '''optimizer.add_param_group({\n",
    "                'params': new_params,\n",
    "                'lr': self.base_lr / 10, # Use a smaller LR for fine-tuning\n",
    "                'weight_decay': self.weight_decay\n",
    "            })'''\n",
    "            # Use a much smaller LR for the fine-tuned backbone layers\n",
    "            low_lr = self.base_lr / 100\n",
    "\n",
    "            print(f\"  -> Added {len(new_params)} backbone params with low LR ({low_lr}).\")\n",
    "            optimizer.add_param_group({\n",
    "                'params': new_params,\n",
    "                'lr': low_lr,\n",
    "                'weight_decay': self.weight_decay\n",
    "            })\n",
    "\n",
    "            print(f\"  -> Added {len(new_params)} new parameters to the optimizer.\")\n",
    "\n",
    "        # 6. Decrement the pointer to the next block and schedule the next unfreeze event\n",
    "        self.next_block_to_unfreeze -= 1\n",
    "        self.next_unfreeze_epoch += self.unfreeze_every\n",
    "\n",
    "class PostWarmupLRScheduler:\n",
    "    def __init__(self, optimizer, base_lr=0.001, rise_epochs=3, weight_decay=1e-4):\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.rise_epochs = rise_epochs\n",
    "        self.epoch_count = 0\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def step(self):\n",
    "        if self.epoch_count >= self.rise_epochs:\n",
    "            return\n",
    "\n",
    "        new_lr = self.base_lr * (self.epoch_count + 1) / self.rise_epochs\n",
    "        for i, group in enumerate(self.optimizer.param_groups):\n",
    "            group['lr'] = new_lr\n",
    "        self.epoch_count += 1\n",
    "\n",
    "        print(f\"üìà LR Increase: Set LR to {new_lr:.6f}\")\n",
    "\n",
    "class HybridLRScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        warmup_epochs,\n",
    "        total_epochs,\n",
    "        mode=\"plateau\",  # ‚úÖ 'cosine' or 'plateau'\n",
    "        plateau_patience=5,\n",
    "        plateau_factor=0.5,\n",
    "        min_lr=1e-6\n",
    "    ):\n",
    "        assert mode in [\"cosine\", \"plateau\"], \"mode must be 'cosine' or 'plateau'\"\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.mode = mode\n",
    "        self.plateau_patience = plateau_patience\n",
    "        self.plateau_factor = plateau_factor\n",
    "        self.min_lr = min_lr\n",
    "        self.lr_history = []\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.epochs_since_improvement = 0\n",
    "\n",
    "        # ‚úÖ Store original learning rates\n",
    "        self.initial_lr = [group['lr'] for group in optimizer.param_groups]\n",
    "\n",
    "    def step(self, val_acc=None):\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        self.lr_history.append(lr)\n",
    "\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # üîº Linear Warmup\n",
    "            scale = (self.current_epoch + 1) / self.warmup_epochs\n",
    "            for i, group in enumerate(self.optimizer.param_groups):\n",
    "                base_lr = self.initial_lr[i] if i < len(self.initial_lr) else group['lr']\n",
    "                group['lr'] = base_lr * scale\n",
    "\n",
    "        elif self.mode == \"cosine\":\n",
    "            # üåÄ Cosine Annealing\n",
    "            progress = (self.current_epoch - self.warmup_epochs) / max(1, self.total_epochs - self.warmup_epochs)\n",
    "            for i, group in enumerate(self.optimizer.param_groups):\n",
    "                base_lr = self.initial_lr[i] if i < len(self.initial_lr) else group['lr']\n",
    "                new_lr = self.min_lr + 0.5 * (base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n",
    "                group['lr'] = new_lr\n",
    "\n",
    "        elif self.mode == \"plateau\":\n",
    "            # üìâ Reduce LR on Plateau\n",
    "            if val_acc is not None:\n",
    "                if val_acc > self.best_val_acc:\n",
    "                    self.best_val_acc = val_acc\n",
    "                    self.epochs_since_improvement = 0\n",
    "                else:\n",
    "                    self.epochs_since_improvement += 1\n",
    "                    if self.epochs_since_improvement >= self.plateau_patience:\n",
    "                        for i, group in enumerate(self.optimizer.param_groups):\n",
    "                            new_lr = max(group['lr'] * self.plateau_factor, self.min_lr)\n",
    "                            group['lr'] = new_lr\n",
    "                            print(f\"Plateau: Reducing LR group {i} to {new_lr:.6f}\")\n",
    "                        self.epochs_since_improvement = 0\n",
    "\n",
    "        self.current_epoch += 1\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "def setup_directories(base_path, model_name, fold=None, attention_type=None):\n",
    "    \"\"\"\n",
    "    Create directory structure:\n",
    "    base_path/fold_{N}_{model_name}_{attention_type}/[checkpoints, weights, graphs, predictions]\n",
    "    \"\"\"\n",
    "    attention_str = str(attention_type).lower() if attention_type else \"none\"\n",
    "    fold_str = f\"fold_{fold}\" if fold is not None else \"fold_None\"\n",
    "    tag = f\"{fold_str}_{model_name}_{attention_str}\"\n",
    "\n",
    "    model_root = os.path.join(base_path, tag)  # ‚úÖ Use tag as subdirectory\n",
    "\n",
    "    checkpoint_dir = os.path.join(model_root, \"checkpoints\")\n",
    "    weights_dir = os.path.join(model_root, \"weights\")\n",
    "    graph_dir = os.path.join(model_root, \"graphs\")\n",
    "    predictions_dir = os.path.join(model_root, \"predictions\")\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "    os.makedirs(graph_dir, exist_ok=True)\n",
    "    os.makedirs(predictions_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{tag}_checkpoint.pth\")\n",
    "    best_weights_path = os.path.join(weights_dir, f\"{tag}_best.pth\")\n",
    "\n",
    "    return checkpoint_path, best_weights_path, graph_dir, predictions_dir\n",
    "\n",
    "def compute_class_mst_alpha_matrix(y_true, y_pred, mst_bins, num_classes=7, num_mst_bins=10, normalize=True):\n",
    "\n",
    "    alpha_matrix = np.ones((num_classes, num_mst_bins), dtype=np.float32)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mst_bins = np.array(mst_bins)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        for mst in range(num_mst_bins):\n",
    "            mask = (y_true == cls) & (mst_bins == mst)\n",
    "            if mask.sum() == 0:\n",
    "                alpha_matrix[cls, mst] = 1.0  # fallback\n",
    "                continue\n",
    "\n",
    "            y_true_subset = y_true[mask]\n",
    "            y_pred_subset = y_pred[mask]\n",
    "\n",
    "            recall = np.sum(y_pred_subset == cls) / (len(y_true_subset) + 1e-6)\n",
    "            alpha_matrix[cls, mst] = 1.0 / (recall + 1e-6)  # inverse recall\n",
    "\n",
    "    if normalize:\n",
    "        # Normalize each row (per class) to sum to num_mst_bins\n",
    "        alpha_matrix = alpha_matrix / alpha_matrix.sum(axis=1, keepdims=True) * num_mst_bins\n",
    "\n",
    "    print(\"üìä Alpha Matrix (class √ó MST):\")\n",
    "    print(np.round(alpha_matrix, 2))\n",
    "\n",
    "    return torch.tensor(alpha_matrix, dtype=torch.float32)\n",
    "\n",
    "def freeze_backbone(model):\n",
    "    for param in model.base.parameters():\n",
    "        param.requires_grad = False\n",
    "    #print(\"üßä Backbone frozen.\")\n",
    "\n",
    "def safe_criterion_call(criterion, outputs, labels, mst_groups=None):\n",
    "    try:\n",
    "        return criterion(outputs, labels, mst_groups)\n",
    "    except TypeError:\n",
    "        return criterion(outputs, labels)\n",
    "\n",
    "def plot_alpha_trends(alpha_history, num_classes, save_path=None):\n",
    "    alpha_array = torch.stack(alpha_history).cpu().numpy()\n",
    "    for class_idx in range(num_classes):\n",
    "        plt.plot(alpha_array[:, class_idx], label=f\"Class {class_idx}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Alpha Weight\")\n",
    "    plt.title(\"Focal Loss Alpha Trend (Inverse Recall)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"üìä Alpha plot saved to: {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def compute_fairness_by_group(y_true, y_probs, class_names, skin_groups=None):\n",
    "    y_preds = np.argmax(y_probs, axis=1)\n",
    "    results = []\n",
    "    if skin_groups is None:\n",
    "        skin_groups = ['unknown'] * len(y_true)\n",
    "    unique_groups = sorted(set(skin_groups))\n",
    "    for group in unique_groups:\n",
    "        indices = [i for i, g in enumerate(skin_groups) if g == group]\n",
    "        if not indices:\n",
    "            continue\n",
    "        group_y_true = [y_true[i] for i in indices]\n",
    "        group_y_pred = [y_preds[i] for i in indices]\n",
    "        results.append({\n",
    "            \"Skin Group\": group,\n",
    "            \"Accuracy\": accuracy_score(group_y_true, group_y_pred),\n",
    "            \"Precision\": precision_score(group_y_true, group_y_pred, average='macro', zero_division=0),\n",
    "            \"Recall\": recall_score(group_y_true, group_y_pred, average='macro', zero_division=0),\n",
    "            \"F1\": f1_score(group_y_true, group_y_pred, average='macro', zero_division=0),\n",
    "        })\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1763671113434,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "MkRE3zrOutj8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# @title MixUp utilities\n",
    "\n",
    "def soft_cross_entropy(pred, soft_targets):\n",
    "    # pred: [B, C] logits; soft_targets: [B, C] probs (rows sum to 1)\n",
    "    log_probs = F.log_softmax(pred, dim=1)\n",
    "    return -(soft_targets * log_probs).sum(dim=1).mean()\n",
    "\n",
    "# ---------- MixUp criterion (soft targets) ----------\n",
    "def mixup_criterion(pred, y_a, y_b, lam, num_classes):\n",
    "    # y_a, y_b: class indices [B]\n",
    "    y_a = F.one_hot(y_a.long(), num_classes=num_classes).float()\n",
    "    y_b = F.one_hot(y_b.long(), num_classes=num_classes).float()\n",
    "    soft_targets = lam * y_a + (1 - lam) * y_b\n",
    "    return soft_cross_entropy(pred, soft_targets)\n",
    "\n",
    "# ---------- Safe MixUp ----------\n",
    "def _to_scalar_alpha(alpha, default=0.4):\n",
    "    \"\"\"\n",
    "    Accepts float/int/0-d tensor/1-d tensor/ndarray; returns a safe positive float.\n",
    "    - If tensor/array with >1 elem (e.g., class weights), use its mean as the scalar alpha.\n",
    "    - Clamps to a small positive to avoid Beta errors.\n",
    "    \"\"\"\n",
    "    if alpha is None:\n",
    "        return float(default)\n",
    "    try:\n",
    "        # turn anything into a tensor on CPU, flatten, take mean -> scalar\n",
    "        a = torch.as_tensor(alpha).detach().float().mean().item()\n",
    "    except Exception:\n",
    "        try:\n",
    "            # last resort\n",
    "            a = float(alpha)\n",
    "        except Exception:\n",
    "            a = float(default)\n",
    "    # clamp to safe range\n",
    "    return float(max(a, 1e-6))\n",
    "\n",
    "def mixup_data(x, y, skin_vec=None, alpha=0.4, epoch=0, warmup_epochs=5, lam_clip=(0.3, 0.7)):\n",
    "    \"\"\"\n",
    "    x: [B, C, H, W]\n",
    "    y: [B]\n",
    "    skin_vec: optional [B, D] or None (if your model ignores it)\n",
    "    alpha: can be scalar or any tensor/array (e.g., class-weight vector) ‚Äî reduced to a scalar\n",
    "    Returns: mixed_x, mixed_skin (or None), y_a, y_b, lam(float)\n",
    "    \"\"\"\n",
    "    if x.ndim != 4:\n",
    "        raise ValueError(f\"Expected x [B,C,H,W], got {tuple(x.shape)}\")\n",
    "    if skin_vec is not None and skin_vec.ndim != 2:\n",
    "        raise ValueError(f\"Expected skin_vec [B,D] or None, got {None if skin_vec is None else tuple(skin_vec.shape)}\")\n",
    "\n",
    "    B = x.size(0)\n",
    "    if B < 2:\n",
    "        # can't permute a single sample; no-op\n",
    "        lam = 1.0\n",
    "        index = torch.arange(B, device=x.device)\n",
    "    else:\n",
    "        if epoch < int(warmup_epochs):\n",
    "            lam = 1.0\n",
    "        else:\n",
    "            a = _to_scalar_alpha(alpha, default=0.4)\n",
    "            # Beta is undefined for non-positive alpha\n",
    "            if a <= 0.0:\n",
    "                lam = 1.0\n",
    "            else:\n",
    "                lam = np.random.beta(a, a)\n",
    "                if lam_clip is not None:\n",
    "                    lo, hi = lam_clip\n",
    "                    lam = float(np.clip(lam, lo, hi))\n",
    "        index = torch.randperm(B, device=x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    mixed_skin = None\n",
    "    if skin_vec is not None:\n",
    "        mixed_skin = lam * skin_vec + (1 - lam) * skin_vec[index]\n",
    "\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, mixed_skin, y_a, y_b, float(lam)\n",
    "\n",
    "\n",
    "def compute_classwise_alpha(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    num_classes=4,\n",
    "    normalize=True,\n",
    "    clip_range=(0.1, 3.0),\n",
    "    prev_alpha=None,\n",
    "    beta=0.9,\n",
    "    smoothing=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute smoothed, capped alpha weights for Focal Loss based on inverse recall.\n",
    "\n",
    "    Args:\n",
    "        y_true (array): Ground truth labels.\n",
    "        y_pred (array): Predicted labels.\n",
    "        num_classes (int): Number of classes.\n",
    "        normalize (bool): Whether to normalize alpha to sum to num_classes.\n",
    "        clip_range (tuple): Min and max values to clip alpha.\n",
    "        prev_alpha (np.ndarray or torch.Tensor): Previous epoch's alpha for smoothing.\n",
    "        beta (float): Smoothing factor for EMA.\n",
    "        smoothing (bool): Whether to apply exponential smoothing.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Alpha weights.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    recalls = cm.diagonal() / (cm.sum(axis=1) + 1e-6)  # Avoid division by zero\n",
    "    alphas = 1.0 / (recalls + 1e-6)\n",
    "\n",
    "    # Clip alpha to avoid extreme weights\n",
    "    alphas = np.clip(alphas, clip_range[0], clip_range[1])\n",
    "\n",
    "    # Smooth with EMA using previous alpha\n",
    "    if smoothing and prev_alpha is not None:\n",
    "        if isinstance(prev_alpha, torch.Tensor):\n",
    "            prev_alpha = prev_alpha.detach().cpu().numpy()\n",
    "        alphas = beta * prev_alpha + (1 - beta) * alphas\n",
    "\n",
    "\n",
    "    # Normalize to keep total scale constant\n",
    "    if normalize:\n",
    "        alphas = alphas / alphas.sum() * num_classes\n",
    "\n",
    "    print(\"üîç Dynamic Alpha (inverse recall):\", np.round(alphas, 4))\n",
    "    return torch.tensor(alphas, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1763671113453,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "xleWxxFxvUNL"
   },
   "outputs": [],
   "source": [
    "# @title Get Model Function\n",
    "\n",
    "def get_model_with_attention(model_name, num_classes, attention_type=\"none\", pretrained=True,\n",
    "                             fold=None, weights_root=None, resume=True, **kwargs):\n",
    "    # Extract any additional configuration arguments passed through kwargs\n",
    "    use_film_before = kwargs.get(\"use_film_before\", False)\n",
    "    use_film_in_cbam = kwargs.get(\"use_film_in_cbam\", False)\n",
    "    use_triplet_embedding = kwargs.get(\"use_triplet_embedding\", False)\n",
    "    triplet_embedding_dim = kwargs.get(\"triplet_embedding_dim\", 512)\n",
    "    include_skin_vec = kwargs.get(\"include_skin_vec\", True)\n",
    "    drop_path_rate = kwargs.get(\"drop_path_rate\", 0.2)\n",
    "    fusion_mode = kwargs.get(\"fusion_mode\", \"concat\")  # Default to \"concat\" if not passed\n",
    "    fusion_hidden_dim = kwargs.get(\"fusion_hidden_dim\", 128)  # Default to 128 if not passed\n",
    "\n",
    "    print(f\"Fusion hidden dim: {fusion_hidden_dim}\")  # Debugging line to check the value\n",
    "\n",
    "    # EfficientNet models\n",
    "    if model_name.startswith(\"efficientnet_b\"):\n",
    "        model = EfficientNetWithAttention(\n",
    "            num_classes=num_classes,\n",
    "            attention_type=attention_type,\n",
    "            pretrained=pretrained,\n",
    "            use_film_before=use_film_before,\n",
    "            use_film_in_cbam=use_film_in_cbam,\n",
    "            use_triplet_embedding=use_triplet_embedding,\n",
    "            triplet_embedding_dim=triplet_embedding_dim,\n",
    "            include_skin_vec=include_skin_vec,\n",
    "            efficientnet_variant=model_name,\n",
    "            fusion_mode=fusion_mode,  # Pass fusion_mode to EfficientNetWithAttention\n",
    "            fusion_hidden_dim=fusion_hidden_dim  # Pass fusion_hidden_dim to EfficientNetWithAttention\n",
    "        )\n",
    "\n",
    "    # ResNet models\n",
    "    elif model_name in [\"resnet101v2\", \"resnet101d\", \"resnet152d\", \"resnetrs101\"]:\n",
    "        model = ResNetWithAttention(\n",
    "            num_classes=num_classes,\n",
    "            backbone_name=model_name,\n",
    "            attention_type=attention_type,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            use_film_before=use_film_before,\n",
    "            use_film_in_cbam=use_film_in_cbam,\n",
    "            use_triplet_embedding=use_triplet_embedding,\n",
    "            triplet_embedding_dim=triplet_embedding_dim,\n",
    "            include_skin_vec=include_skin_vec,\n",
    "            fusion_mode=fusion_mode,  # Pass fusion_mode to ResNetWithAttention\n",
    "            fusion_hidden_dim=fusion_hidden_dim  # Pass fusion_hidden_dim to ResNetWithAttention\n",
    "        )\n",
    "\n",
    "    # InceptionV3 model\n",
    "    elif model_name.lower() in [\"inceptionv3\", \"inception_v3\"]:\n",
    "        model = InceptionV3WithAttention(\n",
    "            num_classes=num_classes,\n",
    "            attention_type=attention_type,\n",
    "            pretrained=pretrained,\n",
    "            use_film_before=use_film_before,\n",
    "            use_film_in_cbam=use_film_in_cbam,\n",
    "            use_triplet_embedding=use_triplet_embedding,\n",
    "            triplet_embedding_dim=triplet_embedding_dim,\n",
    "            include_skin_vec=include_skin_vec,\n",
    "            fusion_mode=fusion_mode,  # Pass fusion_mode to InceptionV3WithAttention\n",
    "            fusion_hidden_dim=fusion_hidden_dim  # Pass fusion_hidden_dim to InceptionV3WithAttention\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1763671113477,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "9KZzdDvVwTBC"
   },
   "outputs": [],
   "source": [
    "# @title Local Train\n",
    "\n",
    "def local_train(\n",
    "    train_loader, model, device, num_epochs=10, lr=0.003,\n",
    "    val_loader=None, save_model_path=None, model_name=\"model\",\n",
    "    fold=None, resume_path=None, alpha=0.2, mixup_enabled=True,\n",
    "    warmup_epochs=4, num_classes=4, attention_type=\"none\",\n",
    "    log_lr_each_epoch=True, y_train=None, use_gradcam=False, # Add use_gradcam flag\n",
    "    triplet_loss_weight=0.1 # --- NEW PARAMETER FOR TRIPLET LOSS WEIGHT ---\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a local training loop with a clean tqdm progress bar and full checkpoint/resume logic.\n",
    "    \"\"\"\n",
    "\n",
    "    # === Corrected Optimizer Setup ===\n",
    "\n",
    "    # 1. Get all \"new\" parameters that are NOT in the backbone\n",
    "    new_params = [\n",
    "        {'params': model.classifier.parameters(), 'lr': lr, 'weight_decay': 1e-4}\n",
    "    ]\n",
    "\n",
    "    # Dynamically add other new parts if they exist\n",
    "    if hasattr(model, 'skin_mlp'):\n",
    "        new_params.append({'params': model.skin_mlp.parameters(), 'lr': lr, 'weight_decay': 1e-4})\n",
    "\n",
    "    if hasattr(model, 'image_proj'):\n",
    "        new_params.append({'params': model.image_proj.parameters(), 'lr': lr, 'weight_decay': 1e-4})\n",
    "\n",
    "    if hasattr(model, 'skin_proj'):\n",
    "        new_params.append({'params': model.skin_proj.parameters(), 'lr': lr, 'weight_decay': 1e-4})\n",
    "\n",
    "    if hasattr(model, 'triplet_proj'):\n",
    "        new_params.append({'params': model.triplet_proj.parameters(), 'lr': lr, 'weight_decay': 1e-4})\n",
    "\n",
    "    if hasattr(model, 'gate'):\n",
    "        new_params.append({'params': model.gate.parameters(), 'lr': lr, 'weight_decay': 1e-4})\n",
    "\n",
    "    if hasattr(model, 'attn') and not isinstance(model.attn, nn.Identity):\n",
    "         new_params.append({'params': model.attn.parameters(), 'lr': lr, 'weight_decay': 1e-4})\n",
    "\n",
    "    if hasattr(model, 'film') and not isinstance(model.film, nn.Identity):\n",
    "         new_params.append({'params': model.film.parameters(), 'lr': lr, 'weight_decay': 1e-4})\n",
    "\n",
    "    print(f\"‚úÖ Optimizing {len(new_params)} new parameter groups with high LR ({lr}).\")\n",
    "\n",
    "    # 2. Create the optimizer *only* with these new parameter groups\n",
    "    optimizer = torch.optim.AdamW(new_params)\n",
    "\n",
    "    if hasattr(model, 'attn') and not isinstance(model.attn, nn.Identity):\n",
    "        # Check if 'attn' parameters are already added\n",
    "        # Iterate over parameters of model.attn and check if any of them are already in optimizer\n",
    "        for param in model.attn.parameters():\n",
    "            if not any(param is group_param for group in optimizer.param_groups for group_param in group['params']):\n",
    "                optimizer.add_param_group({'params': model.attn.parameters(), 'lr': lr, 'weight_decay': 1e-4})\n",
    "                break  # Add param group only once\n",
    "\n",
    "\n",
    "    # History lists for training and validation metrics\n",
    "    train_loss_history, val_loss_history, train_acc_history, val_acc_history, lrs_history = [], [], [], [], []\n",
    "\n",
    "    # Class weights for loss function\n",
    "    class_weights_np = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    weights = torch.tensor(class_weights_np, dtype=torch.float, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
    "\n",
    "    # --- NEW: Triplet Loss Criterion ---\n",
    "    triplet_criterion = InBatchHardTripletLoss(margin=1.0)\n",
    "\n",
    "    # Learning rate scheduler and utilities\n",
    "    scheduler = HybridLRScheduler(optimizer, warmup_epochs=warmup_epochs, total_epochs=num_epochs, mode='cosine', min_lr=1e-6)\n",
    "\n",
    "    # --- START FIX 1: AMP and Scaler Setup ---\n",
    "    scaler = torch.amp.GradScaler() if device.type == \"cuda\" else None\n",
    "    use_amp = (scaler is not None)\n",
    "    # This context will automatically handle both CPU/GPU and enabled/disabled states\n",
    "    autocast_context = torch.amp.autocast(device_type=device.type, enabled=use_amp)\n",
    "    # --- END FIX 1 ---\n",
    "\n",
    "    checkpoint_path, best_weights_path, _, _ = setup_directories(\n",
    "        base_path=save_model_path, model_name=model_name, fold=fold, attention_type=attention_type or \"none\"\n",
    "    )\n",
    "\n",
    "    # Initial state variables for early stopping\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    early_stop_counter = 0\n",
    "    early_stop_patience = 30\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Checkpoint resume logic\n",
    "    if resume_path and os.path.isfile(resume_path):\n",
    "        try:\n",
    "            print(f\"Attempting to resume training from checkpoint: {resume_path}\")\n",
    "            # --- START FIX 2: Added weights_only=False ---\n",
    "            checkpoint = torch.load(resume_path, map_location=device, weights_only=False)\n",
    "            # --- END FIX 2 ---\n",
    "            model.load_state_dict(checkpoint[\"model_state\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "            best_val_accuracy = checkpoint.get(\"best_val_accuracy\", 0.0)\n",
    "            start_epoch = checkpoint.get(\"epoch\", 0) + 1\n",
    "            print(f\"Successfully resumed training from checkpoint at Epoch {start_epoch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load checkpoint from {resume_path}: {e}\")\n",
    "            print(\"Starting training from scratch (Epoch 0) instead.\")\n",
    "            best_val_accuracy, start_epoch, early_stop_counter = 0.0, 0, 0\n",
    "    else:\n",
    "        print(f\"No valid checkpoint found. Starting training from scratch (Epoch 0).\")\n",
    "\n",
    "    # Backbone freezing for warmup\n",
    "    if start_epoch < warmup_epochs:\n",
    "        freeze_backbone(model)\n",
    "\n",
    "    # Gradual unfreezing setup\n",
    "    unfreezer = GradualUnfreezer(model, base_lr=lr, start_epoch=warmup_epochs, unfreeze_every=2, max_blocks=None, weight_decay=1e-4)\n",
    "    lr_riser = PostWarmupLRScheduler(optimizer, base_lr=lr, rise_epochs=3)\n",
    "\n",
    "    # Training loop with TQDM progress bar\n",
    "    epoch_pbar = tqdm(range(start_epoch, num_epochs), desc=f\"Training {model_name}\")\n",
    "\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        if hasattr(train_loader.dataset, 'set_epoch'):\n",
    "            train_loader.dataset.set_epoch(epoch)\n",
    "\n",
    "        unfreezer.step(optimizer, epoch + 1)\n",
    "        lr_riser.step()\n",
    "        lrs_history.append(scheduler.get_lr()[0])\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Unpack the 7 items from the dataset\n",
    "            images, labels, skin_vecs, triplet, mst_bin, skin_group, metadata = batch\n",
    "            images, labels, skin_vecs, triplet = images.to(device), labels.to(device), skin_vecs.to(device), triplet.to(device)\n",
    "\n",
    "            # Mixup logic\n",
    "            use_mixup = False\n",
    "            if mixup_enabled:\n",
    "                mix_ratio = max(1.0 - epoch / num_epochs, 0.1)\n",
    "                images, skin_vecs, y_a, y_b, lam = mixup_data(images, labels, skin_vecs, weights * mix_ratio, epoch, warmup_epochs)\n",
    "                use_mixup = True\n",
    "            else:\n",
    "                y_a, y_b, lam = labels, labels, 1.0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # --- START FIX 3: Conditional backward pass ---\n",
    "            try:\n",
    "                # Forward pass with autocast\n",
    "                with autocast_context:\n",
    "                    # --- MODIFICATION: Get features for Triplet Loss ---\n",
    "                    out, feat = model(images, skin_vec=skin_vecs, triplet_embedding=triplet, return_features=True)\n",
    "                    classification_loss = mixup_criterion(out, y_a, y_b, lam, num_classes=num_classes) if use_mixup else criterion(out, labels)\n",
    "\n",
    "                    # --- NEW: Calculate Triplet Loss ---\n",
    "                    t_loss = triplet_criterion(feat, labels)\n",
    "                    loss = classification_loss + triplet_loss_weight * t_loss\n",
    "\n",
    "\n",
    "                # Backward pass\n",
    "                if use_amp:\n",
    "                    # CUDA / AMP path\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    # Standard CPU path\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"üö® STEP FAILURE: {e}\")\n",
    "                traceback.print_exc()\n",
    "                raise\n",
    "            # --- END FIX 3 ---\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            correct += (out.argmax(dim=1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # Handle case where train_loader was empty (e.g., drop_last=True)\n",
    "        if total == 0:\n",
    "            print(f\"‚ö†Ô∏è Epoch {epoch}: No data processed in training loop. Check DataLoader and batch_size.\")\n",
    "            avg_train_loss, train_acc = 0.0, 0.0\n",
    "        else:\n",
    "            avg_train_loss, train_acc = total_loss / total, correct / total\n",
    "\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        avg_val_loss, val_acc = 0.0, 0.0\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_correct, val_total, val_loss_total = 0, 0, 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    # Unpack the 7 items for validation\n",
    "                    v_images, v_labels, v_skin, v_triplet, v_mst_bin, v_skin_group, v_metadata = batch\n",
    "                    v_images, v_labels, v_skin, v_triplet = v_images.to(device), v_labels.to(device), v_skin.to(device), v_triplet.to(device)\n",
    "\n",
    "                    # Use autocast context\n",
    "                    with autocast_context:\n",
    "                        # --- MODIFICATION: Get features for Triplet Loss (not used in val loss, but consistency) ---\n",
    "                        out, feat = model(v_images, skin_vec=v_skin, triplet_embedding=v_triplet, return_features=True)\n",
    "                        val_classification_loss = criterion(out, v_labels)\n",
    "\n",
    "                        # --- NEW: Calculate Triplet Loss for validation (for logging, not backprop) ---\n",
    "                        val_t_loss = triplet_criterion(feat, v_labels)\n",
    "                        val_loss = val_classification_loss + triplet_loss_weight * val_t_loss\n",
    "\n",
    "                    val_loss_total += val_loss.item() * v_labels.size(0)\n",
    "                    val_correct += (out.argmax(dim=1) == v_labels).sum().item()\n",
    "                    val_total += v_labels.size(0)\n",
    "\n",
    "            if val_total == 0:\n",
    "                print(f\"‚ö†Ô∏è Epoch {epoch}: No data processed in validation loop.\")\n",
    "                avg_val_loss, val_acc = 0.0, 0.0\n",
    "            else:\n",
    "                avg_val_loss, val_acc = val_loss_total / val_total, val_correct / val_total\n",
    "\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "            val_acc_history.append(val_acc)\n",
    "            scheduler.step(val_acc)\n",
    "\n",
    "        # Update the progress bar\n",
    "        epoch_pbar.set_postfix(train_loss=f\"{avg_train_loss:.4f}\", train_acc=f\"{train_acc:.4f}\", val_loss=f\"{avg_val_loss:.4f}\", val_acc=f\"{val_acc:.4f}\", lr=f\"{scheduler.get_lr()[0]:.1e}\")\n",
    "\n",
    "        # Early stopping & checkpoint logic\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {early_stop_patience} epochs with no improvement.\")\n",
    "                break\n",
    "\n",
    "        if checkpoint_path:\n",
    "            # Ensure the parent directory exists right before saving\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            torch.save({\"epoch\": epoch, \"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict(), \"best_val_accuracy\": best_val_accuracy}, checkpoint_path)\n",
    "\n",
    "    # End of training loop\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    if best_weights_path:\n",
    "        torch.save(best_model_weights, best_weights_path)\n",
    "        print(f\"\\n‚úÖ Saved best model weights to: {best_weights_path}\")\n",
    "\n",
    "    # Don't try to empty cache if CUDA is not available\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return model, {\"train_loss\": train_loss_history, \"val_loss\": val_loss_history, \"train_acc\": train_acc_history, \"val_acc\": val_acc_history, \"lrs\": lrs_history}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763671113480,
     "user": {
      "displayName": "Ticauris Stokes",
      "userId": "13802650413118303337"
     },
     "user_tz": 360
    },
    "id": "XrP3SVmdviDi"
   },
   "outputs": [],
   "source": [
    "# @title K-Fold\n",
    "\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "\n",
    "def kfold_cross_validation(\n",
    "    X, y, z, label_encoder, model_names, attention_types, num_classes,\n",
    "    transform, num_folds=5, num_epochs=10, batch_size=64,\n",
    "    save_root=RESULTS_DIR, triplet_embedding_dict=None, val_size=0.3,\n",
    "    triplet_loss_weight=0.1 # --- ADDED: Triplet Loss Weight parameter ---\n",
    "):\n",
    "    device = DEVICE\n",
    "    seed = np.random.randint(0, 99999)\n",
    "    print(f\"Using random_state = {seed} for this k-fold trial\")\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=num_folds, test_size=val_size, random_state=seed)\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X, y)):\n",
    "        current_fold_num = fold_idx + 1\n",
    "        print(f\"\\n‚ÑÄ Fold {current_fold_num}/{num_folds}\")\n",
    "\n",
    "        X_tr, y_tr_orig, z_tr = [X[i] for i in train_idx], [y[i] for i in train_idx], [z[i] for i in train_idx]\n",
    "        X_val, y_val_orig, z_val = [X[i] for i in val_idx], [y[i] for i in val_idx], [z[i] for i in val_idx]\n",
    "        print(f\"Train set size: {len(X_tr)} | Val set size: {len(X_val)}\")\n",
    "\n",
    "        fold_classes = sorted(set(y_tr_orig + y_val_orig))\n",
    "        fold_num_classes = len(fold_classes)\n",
    "        class_mapping = {label: idx for idx, label in enumerate(fold_classes)}\n",
    "        y_tr = [class_mapping[lbl] for lbl in y_tr_orig]\n",
    "        y_val = [class_mapping[lbl] for lbl in y_val_orig]\n",
    "\n",
    "        print(f\"Fold {current_fold_num} Classes: {fold_classes} ‚Üí Remapped to: {list(class_mapping.values())}\")\n",
    "        print(f\"√∞≈∏‚Äô‚Ä∞ Fold {current_fold_num} Class Distribution (Train): {dict(Counter(y_tr))}\")\n",
    "        print(f\"√∞≈∏‚Äô‚Ä∞ Fold {current_fold_num} Class Distribution (Val): {dict(Counter(y_val))}\")\n",
    "\n",
    "        CLASS_POLICY_MAP = {\n",
    "            0: \"standard_transform\",\n",
    "            1: \"standard_transform\",\n",
    "            2: \"standard_transform\",\n",
    "            3: \"aggressive_transform\",\n",
    "            4: \"standard_transform\",\n",
    "            5: \"aggressive_transform\",\n",
    "            6: \"standard_transform\",\n",
    "        }\n",
    "\n",
    "        train_dataset = CustomDataset(\n",
    "            image_paths=X_tr,\n",
    "            labels=y_tr,\n",
    "            metadata=z_tr,\n",
    "            include_skin_vec=True,\n",
    "            triplet_embedding_dict=triplet_embedding_dict,\n",
    "            class_policy_map=CLASS_POLICY_MAP, # Pass class_policy_map for training\n",
    "            num_classes=fold_num_classes\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            # num_workers=4,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        val_dataset = CustomDataset(\n",
    "            image_paths=X_val,\n",
    "            labels=y_val,\n",
    "            metadata=z_val,\n",
    "            include_skin_vec=True,\n",
    "            triplet_embedding_dict=triplet_embedding_dict,\n",
    "            transform_name=\"standard_transform\", # Pass transform_name for validation\n",
    "            num_classes=fold_num_classes\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            #num_workers=4,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        train_dataset.set_epoch(0)\n",
    "\n",
    "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "\n",
    "        for attn_type in attention_types:\n",
    "            for model_name in model_names:\n",
    "                run_name = f\"{model_name}_{attn_type}\"\n",
    "                print(f\"\\nTraining: {run_name.upper()} ‚Äî Fold {current_fold_num}\")\n",
    "\n",
    "                checkpoint_path, best_weights_path, graph_dir, predictions_dir = setup_directories(\n",
    "                    base_path=save_root,\n",
    "                    model_name=model_name,\n",
    "                    fold=current_fold_num,\n",
    "                    attention_type=attn_type\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    model = get_model_with_attention(\n",
    "                        model_name=model_name, num_classes=fold_num_classes, attention_type=attn_type,\n",
    "                        pretrained=True, fold=current_fold_num, weights_root=save_root, resume=True,\n",
    "                        use_film_before=True, use_film_in_cbam=True, use_triplet_embedding=True,\n",
    "                        triplet_embedding_dim=512, fusion_mode=\"concat\"\n",
    "                    ).to(device)\n",
    "                    #print(f\"Before local train fusion_mode: {model.fusion_mode}, fusion_hidden_dim: {model.fusion_hidden_dim}\")\n",
    "                    model, training_history_data = local_train(\n",
    "                        train_loader=train_loader, model=model, device=device, num_epochs=num_epochs,\n",
    "                        lr=0.001, val_loader=val_loader, save_model_path=save_root,\n",
    "                        model_name=model_name, fold=current_fold_num, resume_path=checkpoint_path,\n",
    "                        alpha=0.3, mixup_enabled=True, warmup_epochs=5,\n",
    "                        num_classes=fold_num_classes, attention_type=attn_type, y_train=y_tr,\n",
    "                        triplet_loss_weight=triplet_loss_weight # --- ADDED: Pass triplet_loss_weight ---\n",
    "                    )\n",
    "                    #print(f\"In local train fusion_mode: {model.fusion_mode}, fusion_hidden_dim: {model.fusion_hidden_dim}\")\n",
    "                    gradcam_layer = get_gradcam_layer(model, model_name)\n",
    "                    evaluate_model(\n",
    "                        model=model, test_loader=val_loader, device=device,\n",
    "                        label_encoder=label_encoder,\n",
    "                        save_dir=predictions_dir,\n",
    "                        model_name=f\"{model_name}_{attn_type}_fold{current_fold_num}\",\n",
    "                        visualize_gradcam=True, gradcam_layer=gradcam_layer, graph_dir=graph_dir,\n",
    "                        save_training_curves=True, training_curves_data=training_history_data,\n",
    "                        fold_classes=fold_classes,\n",
    "                        plot_tsne_enabled=True\n",
    "                    )\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Training and evaluation time for {run_name.upper()} ‚Äî Fold {current_fold_num}: {elapsed:.2f} seconds\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error ‚Äî Skipping {run_name.upper()} (Fold {current_fold_num}): {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                finally:\n",
    "                    if 'model' in locals():\n",
    "                        del model\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "983638517d0e412980d5fbf46cecf6de",
      "2739692bfe9942f1a4e0c7117e552f44",
      "40f83e2473764ef8bfe840a68d8d335c",
      "b645bb3f38c14ca5a469e5d2328a70c2",
      "45b89bd75471448abf32c03c7b1cd92b",
      "55fe74ff925b4db2915d4a32a4b2fe3e",
      "5cdeeccbba9d43448decd05b4a2c7011",
      "4b8278567671472086430f3c7e71ebc0",
      "72c901c794804e06b1343efda65b1a4e",
      "62eb82ad61c54f21805d3dc87ff05fc3",
      "510426412e244155906de8c808f53ca1",
      "140e95e7f4914b14be2e7481be65bb4d",
      "7851c2556b2f4fd9bd84d280dabef0e0",
      "44936180bb124f958ea68898cc4cd0a6",
      "0720809942e64a659edb8384457e4730",
      "008b7d3c1a6b4642bffb24af2d04db9d",
      "9b4012dec7814f789d74c5182168811b",
      "a729282a210f40a1b174afbc4a89cd9f",
      "8762d9055a244c6caccddaa5f44a4719",
      "f845459d961d447c87a385ee1c2b175f",
      "bfe2a378627443fca0fd68097db98ef9",
      "72bc1f1671674fa0ae1e7022bfafdf40",
      "b12ebcb1b0604a83887d76c40dafdb54",
      "bcb3df9a625a430d9fc8e2b6baa554de",
      "9b68812443464d7baebfea3afad56a4b",
      "7cfd94f6021c4174bfb7662632eee00a",
      "636954299a094614bfa2316dde16f025",
      "a2330bc02e2745ad9dd6ec3a2574d428",
      "390a384414ac4a1394d711a9d1009643",
      "14bba48ee11947ce836e060ee9c36cb1",
      "b3b066f97325409b8d9cfd156b9c2597",
      "f85730616cb445ea9642ca1190ae41da",
      "59dbd9c96f99422c823126175a08f63c"
     ]
    },
    "id": "xAKIfi6jwlTF",
    "outputId": "a7d260db-93c1-4215-9e03-bbdb4043f20f"
   },
   "outputs": [],
   "source": [
    "# @title Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === üîß Configuration ===\n",
    "    train_dataset_dir = BASE_DATASET_DIR\n",
    "    save_model_root = RESULTS_DIR\n",
    "    triplet_path = EMBED_NPY\n",
    "\n",
    "    num_epochs = 50\n",
    "    batch_size = 64\n",
    "    n_splits = 1\n",
    "    model_names = [\"efficientnet_b0\",\"InceptionV3\", \"resnet152d\"] #, \"InceptionV3\", \"resnet152d\"\n",
    "    attention_types = [\"cbam\"]\n",
    "\n",
    "    # --- Triplet Loss Weight ---\n",
    "    triplet_loss_weight = 0.1 # Adjust this weight as needed\n",
    "\n",
    "    # === üìÇ 1. Load and Process Training Data ===\n",
    "    print(\"--- Loading and Processing Training Data ---\")\n",
    "    X_train_paths, y_train_labels = load_img_from_dir(train_dataset_dir, max_images_per_class=2500)\n",
    "    if not X_train_paths:\n",
    "        raise RuntimeError(f\"No training images found in: {train_dataset_dir}\")\n",
    "\n",
    "    triplet_embedding_dict = torch.load(triplet_path, map_location='cpu')\n",
    "\n",
    "    # Create the final data lists in a single, efficient loop\n",
    "    X_train, y_train, z_train = [], [], []\n",
    "    for path, label in zip(X_train_paths, y_train_labels):\n",
    "        if os.path.basename(path).lower() in triplet_embedding_dict:\n",
    "            color_metrics = extract_color_metrics_and_estimate_mst(path)\n",
    "            if color_metrics and 1 <= color_metrics.get(\"MST\", 0) <= 10:\n",
    "                X_train.append(path)\n",
    "                y_train.append(label)\n",
    "                z_train.append(color_metrics)\n",
    "\n",
    "    print(f\"Total usable training images: {len(X_train)}\")\n",
    "\n",
    "    # === Encode Labels ===\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    # Calculate dynamic target counts based on a percentage of the largest (class, MST) group size\n",
    "    #dynamic_target_counts = calculate_dynamic_target_counts(y_train_encoded, z_train, oversample_percentage=1.2)\n",
    "\n",
    "    # Balance the dataset using dynamic target counts\n",
    "    #X_train, y_train, z_train = balance_data_to_targets(X_train, y_train_encoded, z_train, dynamic_target_counts)\n",
    "\n",
    "    # === üîÅ 3. Run K-fold Cross-Validation on the Training Set ===\n",
    "    kfold_cross_validation(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        z=z_train,\n",
    "        label_encoder=label_encoder,\n",
    "        model_names=model_names,\n",
    "        attention_types=attention_types,\n",
    "        num_classes=num_classes,\n",
    "        transform=None,\n",
    "        num_folds=n_splits,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        save_root=save_model_root,\n",
    "        triplet_embedding_dict=triplet_embedding_dict,\n",
    "        triplet_loss_weight=triplet_loss_weight\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ K-fold cross-validation and graph generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOQOKnIqWPOYkaQvDHh6Irb",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "008b7d3c1a6b4642bffb24af2d04db9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0720809942e64a659edb8384457e4730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfe2a378627443fca0fd68097db98ef9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_72bc1f1671674fa0ae1e7022bfafdf40",
      "value": "‚Äá95.5M/95.5M‚Äá[00:01&lt;00:00,‚Äá73.9MB/s]"
     }
    },
    "140e95e7f4914b14be2e7481be65bb4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7851c2556b2f4fd9bd84d280dabef0e0",
       "IPY_MODEL_44936180bb124f958ea68898cc4cd0a6",
       "IPY_MODEL_0720809942e64a659edb8384457e4730"
      ],
      "layout": "IPY_MODEL_008b7d3c1a6b4642bffb24af2d04db9d"
     }
    },
    "14bba48ee11947ce836e060ee9c36cb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2739692bfe9942f1a4e0c7117e552f44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55fe74ff925b4db2915d4a32a4b2fe3e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5cdeeccbba9d43448decd05b4a2c7011",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "390a384414ac4a1394d711a9d1009643": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40f83e2473764ef8bfe840a68d8d335c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b8278567671472086430f3c7e71ebc0",
      "max": 21355344,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_72c901c794804e06b1343efda65b1a4e",
      "value": 21355344
     }
    },
    "44936180bb124f958ea68898cc4cd0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8762d9055a244c6caccddaa5f44a4719",
      "max": 95533832,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f845459d961d447c87a385ee1c2b175f",
      "value": 95533832
     }
    },
    "45b89bd75471448abf32c03c7b1cd92b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b8278567671472086430f3c7e71ebc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "510426412e244155906de8c808f53ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55fe74ff925b4db2915d4a32a4b2fe3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59dbd9c96f99422c823126175a08f63c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5cdeeccbba9d43448decd05b4a2c7011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62eb82ad61c54f21805d3dc87ff05fc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "636954299a094614bfa2316dde16f025": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72bc1f1671674fa0ae1e7022bfafdf40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72c901c794804e06b1343efda65b1a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7851c2556b2f4fd9bd84d280dabef0e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b4012dec7814f789d74c5182168811b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a729282a210f40a1b174afbc4a89cd9f",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "7cfd94f6021c4174bfb7662632eee00a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f85730616cb445ea9642ca1190ae41da",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_59dbd9c96f99422c823126175a08f63c",
      "value": "‚Äá242M/242M‚Äá[00:13&lt;00:00,‚Äá30.6MB/s]"
     }
    },
    "8762d9055a244c6caccddaa5f44a4719": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "983638517d0e412980d5fbf46cecf6de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2739692bfe9942f1a4e0c7117e552f44",
       "IPY_MODEL_40f83e2473764ef8bfe840a68d8d335c",
       "IPY_MODEL_b645bb3f38c14ca5a469e5d2328a70c2"
      ],
      "layout": "IPY_MODEL_45b89bd75471448abf32c03c7b1cd92b"
     }
    },
    "9b4012dec7814f789d74c5182168811b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b68812443464d7baebfea3afad56a4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14bba48ee11947ce836e060ee9c36cb1",
      "max": 241541916,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3b066f97325409b8d9cfd156b9c2597",
      "value": 241541916
     }
    },
    "a2330bc02e2745ad9dd6ec3a2574d428": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a729282a210f40a1b174afbc4a89cd9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b12ebcb1b0604a83887d76c40dafdb54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcb3df9a625a430d9fc8e2b6baa554de",
       "IPY_MODEL_9b68812443464d7baebfea3afad56a4b",
       "IPY_MODEL_7cfd94f6021c4174bfb7662632eee00a"
      ],
      "layout": "IPY_MODEL_636954299a094614bfa2316dde16f025"
     }
    },
    "b3b066f97325409b8d9cfd156b9c2597": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b645bb3f38c14ca5a469e5d2328a70c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62eb82ad61c54f21805d3dc87ff05fc3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_510426412e244155906de8c808f53ca1",
      "value": "‚Äá21.4M/21.4M‚Äá[00:01&lt;00:00,‚Äá14.1kB/s]"
     }
    },
    "bcb3df9a625a430d9fc8e2b6baa554de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2330bc02e2745ad9dd6ec3a2574d428",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_390a384414ac4a1394d711a9d1009643",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "bfe2a378627443fca0fd68097db98ef9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f845459d961d447c87a385ee1c2b175f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f85730616cb445ea9642ca1190ae41da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
